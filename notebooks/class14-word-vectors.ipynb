{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Embedding Models: word2vec #\n",
    "\n",
    "*Lauren F. Klein wrote version 1.0 of this notebook in 2019 based on the [Advanced Topics in Word Vectors workshop](https://dh2018.adho.org/en/machine-reading-part-ii-advanced-topics-in-word-vectors/) at DH 2018 as well as tutorials by [Radim Rehurek](https://rare-technologies.com/word2vec-tutorial/) and [Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). It was updated again in 2021.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With thanks to Matt T.:\n",
    "\n",
    "![oprah vector](http://lklein.com/wp-content/uploads/2021/10/oprah-everyone-3.png)\n",
    "\n",
    "### Everything gets a vector! ###\n",
    "\n",
    "We've actually already been exploring vectors involving words: consider scikit-learn's `CountVectorizer()`, for example, which we used to create the document-term matrix for our tf-idf calculations. That looked at words in relation to the documents in which they appeared. \n",
    "\n",
    "Today, however, we're going to look at words in relation to all other words in a corpus. The vectors that describe these types of relations are called, appropriately enough, *word vectors*. (And sometimes also *word embeddings*).\n",
    "\n",
    "### What is a word vector? ###\n",
    "\n",
    "A *word vector* or *word embedding* is a numerical representation of a word within a corpus, based on co-occurence with other words. Linguists have found that much of the meaning of a word can be derived from looking at its surrounding context. \n",
    "\n",
    "As a way into this idea, let's take a look at [an example of word vectors in action](http://benschmidt.org/profGender/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Word2Vec? ###\n",
    "\n",
    "Word2vec is one popular approach to representing words in this numerical format. It's what Ben Schmidt used to make his website. Conveniently, word2vec is implemented in gensim, which we used last class for topic modeling, so we can use that library again.\n",
    "\n",
    "Word2Vec is a *neural-network* or *deep learning* based approach of generating word vectors. \n",
    "\n",
    "There are many resources out there that will go into the heavy details of deep learning in general or deep learning for NLP such as Yoav Goldberg's Neural Network Methods in Natural Language Processing (Morgan & Claypool Publishers, 2017). Today, you'll get a high level overview -- just enough for you to understand what w2v is doing. \n",
    "\n",
    "### A Picture for Reference ###\n",
    "\n",
    "Before we get into the details of neural networks and deep learning, let's take a quick look at an image that may help anchor some of the more heady concepts we're about to discuss. This shows us the word pairs for a tiny corpus, consisting of a single sentence, \"The quick brown fox jumps over the lazy dog.” \n",
    "\n",
    "The words pairs from this sentence will constitute our training data: what we will use to generate our word vectors. I’ve used a small window size of 2 just for the example. Most of the time the window size will be slightly longer, like 5. In any case, the word highlighted in blue is the input word.\n",
    "\n",
    "![skip-grams](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "The neural network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (“brown”, “fox”) than it is of (“brown”, “unicorn”). When the training is finished, if you give it the word “brown” as input, then it will output a much higher probability for “fox” or “bear” than it will for “unicorn”.\n",
    "\n",
    "But how are these probabilities generated, and what is a neural network anyway? Let's take a minute to talk through these ideas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a neural network? ###\n",
    "\n",
    "Here's great explainer adapted from Jer Thorp's recent book, [Living in Data](https://www.jerthorp.com/)\n",
    "\n",
    "\"A neural network is a mathematical model that traces its roots back to the 1940s, when logicians and neuroscientists and cyberneticians were trying to explain how the human brain learns. Taking a page from how actual neurons cells work, neural net pioneers conceived of a system of nodes, each a kind of simplified neuron. These notes hold a number, called an activation potential, above which a node will \"fire,\" sending a signal to one or more other nodes; or, if there is no node to talk to, spitting out a result. Stitched together into collections (networks), these nodes showed the ability to recognize pattern; that is, to take a specific set of numeric inputs and to turn it, consistently, into an expected result.\"\n",
    "\n",
    "\"Imagine a group of thirteen children in a classroom, sitting in three neat rows of four, with one sitting alone in the back row. Each child can be given either a coookie or a nap. A cookie increases the kid's energy level by one; a nap reduces it by one. If a child's energy goes above a level ten, they have a tantrum, exhausting their excitement but also passing some of it on to any kids they may be connected to. Neural networks tend to be \"feed-forward\" meaning that the signal can only go in one direection from node to node. In our classroom, we can take this to mean that kids can pass energy back only to those sitting behind them.\"\n",
    "\n",
    "\"If we feed a plate of cookies to the kids in the front row, we can expect a wave of hysterics to pass from the front to the back of the class, ending with our lonely back-row student in tears. If every child in the front row got the same amount of sugar, and if they all had the same tolerance for it, this wave would be uniform, starting and ending with crying kids. Neural networks function the way they do, though, because the nodes aren't uniform; they are weighted. This meanas eveyr kid in our class has a different tolerance for cookies, a different level at which they'll break into a conniption. The wave of tears won't flow evenly from front to back, and the signal that we pass into the front won't be the same as the onen that comes out the back.\"\n",
    "\n",
    "\"Asuming the kids in the class are randomly weighted, each with a unique combination of patience and metabolism, feeding different numbers of cookies to the four kids in the front row would result in times when the back-row student loses their temper and times when they don't. Importantly, feeding the same pattern of cookies to the front of the class will always result in the same outcome in the back. This means that the classroom acts together as a pattern-recognition machine. Anything we might be able to translate into 'cookie language,\" a set of four numbers, can be fed into the machine to get a tantrum-based yes or no.\"\n",
    "\n",
    "If the teacher wanted to make sure they got a specific reation from a specific piece of cookie code, they could reseat or replace the students, feeding the cookies to the front until the teacher saw the answer they wanted from the back. The teacher might train the class to recognize their birth year--2015--or the first four notes in \"Baby Shark,\" or a binary representation for the number twelve. In a school assembly, with many more students, this same system could be arranged to recognize bigger sets of numbers, digitized words, or pixelated faces. More than that, a large network might be trained to recognize signals that are similar: faces that are smiling or words that rhyme with \"cheese.\" A crucial point here is that the kids in the network don't need to know anything about the signal or the desired output. They just eat cookies, cry, nap, and compute.\"\n",
    "\n",
    "Of course, we're not training a kids-and-cookies neural network. We're training a computational one with a far greater number of nodes. \n",
    "\n",
    "The important takeaway here, aside from how neural networks go from input to output, is that neural networks are not algorithms in themselves. They just go from start to finish. In order to make use of the input and output, they are most usually paired with algorithms that *train* the network, improving its performance over multiple iterations. \n",
    "\n",
    "Which brings us back to the word2vec algorithm and how it trains the neural network at its core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the neural network ### \n",
    "\n",
    "Here, we’re going to train our neural network to do something more complicated than predict whether a kid at the back of the room will cry or not. Our task is this: given a specific word in the middle of a sentence (the input word--like \"brown,\" as in the image above), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\n",
    "\n",
    "Let's look at our image again:\n",
    "\n",
    "![skip-grams](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "\n",
    "So \"nearby\" is actually defined by the \"window size\" parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total). But in the image the window size is 2. \n",
    "\n",
    "We train the neural network to do this by feeding these word pairs, and he neural network is going to learn the statistics from the number of times each pairing shows up. \n",
    "\n",
    "**NOTE:** I've described something called the skim-gram methods of generating word vectors. THere's also another popular method called CBOW (continuous bag of words). The main difference is that while skip gram learns vectors by predicting the context words that come before and after our given word $w$, CBOW predicts the center word $w$ given context words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One more wrinkle / bonus  ###\n",
    "\n",
    "One last wrinkle in the word2vec process is that, in the end, we’re not actually interested in the predictions generated by the model. What we're interested in is the weights of the nodes of the network itself. These are the actual \"word vectors\" that we want to work with. \n",
    "\n",
    "We can access them fairly easily because word2vec has only a single hidden (or \"projection\") layer, as displayed in the image below. \n",
    "\n",
    "![neural network](http://lklein.com/wp-content/uploads/2019/10/mikolov.png)\n",
    "\n",
    "Conveniently, all you need is one hidden layer for a neural network to be classified as a \"deep\" network. So we're doing deep learning! Fancy!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import gensim, nltk tokenizers, glob, and Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim # remember this! \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we need to create our corpus.\n",
    "# we'll use a new version of the Emory Wheel corpus, which I've cleaned a bit more since last time\n",
    "\n",
    "import os\n",
    "\n",
    "# base_dir = \"../QTM340-Fall21/corpora/wheel-clean/\" \n",
    "base_dir = \"../corpora/wheel-clean/\" \n",
    "\n",
    "all_docs = [] # our list which will store the text of each doc; empty for now\n",
    "\n",
    "docs = os.listdir(base_dir) # get a list of all the files in the directory\n",
    "\n",
    "for doc in docs: # iterate through the docs\n",
    "    if not doc.startswith('.'): # get only the .txt files\n",
    "        with open(base_dir + doc, \"r\", encoding=\"utf-8\") as file: # force unicode conversion to keep PCs happy\n",
    "            text = file.read() # read in the file as a single text string\n",
    "            all_docs.append(text) # append it to the all_docs list\n",
    "\n",
    "# lastly, just check the length of all_docs to see if it's 147\n",
    "len(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim takes text at the level of `sentences` as its input. \n",
    "\n",
    "So let's define a function that a takes a list of texts (e.g. our all_docs list) and converts it into sentences for gensim word2vec to use. The function will lower-case text and tokenize by sentence and word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need our handy nltk tokenizer \n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "# and we'll get titles\n",
    "# directory = \"../QTM340-Fall21/corpora/wheel-clean/\" \n",
    "directory = \"../corpora/wheel-clean/\"\n",
    "files = glob.glob(f\"{directory}/*.txt\")\n",
    "wheel_titles = [Path(file).stem for file in files]\n",
    "\n",
    "# and the function\n",
    "def make_sentences(list_txt):\n",
    "    all_txt = []\n",
    "    counter = 0\n",
    "    for txt in list_txt:\n",
    "        lower_txt = txt.lower()\n",
    "        sentences = sent_tokenize(lower_txt)\n",
    "        sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        all_txt += sentences\n",
    "        print(wheel_titles[counter]) # let's print the title of the article\n",
    "        print(\"Sentences: \" + str(len(sentences)))  # let's check how many sentences there are per article\n",
    "        counter += 1\n",
    "    return all_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's run it\n",
    "sentences = make_sentences(all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our corpus ready for gensim, we can train the model. To do so, we call the function `gensim.models.Word2Vec()`. This function has a couple dozen parameters, some of which are more important than others.\n",
    "\n",
    "Here are a few major ones. Only two are MANDATORY: these are marked with an asterisk:\n",
    "\n",
    "1. `sentences*`: This is where you provide your data. It must be in a format of iterable of iterables.\n",
    "2. `sg`: Your choice of training algorithm. There are two standard ways of training W2V vectors -- 'skipgram' and 'CBOW'. If you enter 1 here the skip-gram is applied; otherwise, the default is CBOW.\n",
    "3. `size*`: This is the length of your resulting word vectors. If you have a large corpus (>few billion tokens) you can go up to 100-300 dimensions. Generally word vectors with more dimensions give better results.\n",
    "4. `window`: This is the window of context words you are training on. In other words, how many words come before and after your given word. A good number is 4 here but this can vary depending on what you are interested in. For instance, if you are more interested in embeddings that embody semantic meaning, smaller window sizes work better. \n",
    "5. `alpha`: The learning rate of your model. If you are interested in machine learning experimentation with your vectors you may experiment with this parameter.\n",
    "6. `seed` (int): This is the random seed for your random initialization. All deep learning models initialize the weights with random floats before training. This is a useful field if you want to replicate your experiments because giving this a seed will initialize 'randomly' deterministically.\n",
    "7. `min_count`: This is the minimum frequency threshold. If a given word appears with lower frequency than provided it will be ignored. This is here because words with very low frequency are hard to train.\n",
    "8. `epochs`: This is the number of iterations (entire run) over the corpus, also known as epochs. Default is 5. Usually anything between 1-10 is ok. The trade offs are that if you have higher iterations, it will take longer to train and the model may overfit on your dataset. However, longer training will allow your vectors to perform better on tasks relevant to your dataset.\n",
    "\n",
    "Most of these settings will not concern us. As you'll see below, we are only going to use four arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train our model!\n",
    "wheel_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=2, # default is 5; this trims the corpus for words only used once; \n",
    "    vector_size=100, # size of NN layers; default is 100; higher for larger corpora\n",
    "    workers=5) # parallel processing; needs Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! We have a trained word2vec model: `wheel_model`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model — and load it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's often useful to save your trained model to disk so that you can reload it as needed. This is very similar syntax to saving topic models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheel_model.save('wheel_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can load a model in the same way (remember this from our topic model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_model = gensim.models.Word2Vec.load('wheel_model') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec can tell us which words, according to its model, are most similar to any other. We call `model.wv.most_similar(\"word\", topn=number of similar words)`. Let's try `class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing some basic functions\n",
    "\n",
    "# basic similarity\n",
    "wheel_model.wv.most_similar(\"class\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think about the results. `class` and `course` make intuitive sense: they have similar meanings to each other. But what's `study`? It's something you do for a class.\n",
    "\n",
    "Our model's parallel invites our critical analysis: rather than take these results as *true*, we need to reflect, in part through turning to articles themselves and close reading them, on how the model arrives at its results, and what they might imply.\n",
    "\n",
    "*Remember: everything the model knows it knows from our corpus. What we're learning are assumptions immanent to the corpus.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "\n",
    "**Copy the code above and test out some words until you find one that has some interesting (or problematic) similar words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between two words\n",
    "\n",
    "We can choose specific words to compare with `model.wv.similarity(w1=\"word_one\",w2=\"word_two\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity b/t two words\n",
    "\n",
    "print(wheel_model.wv.similarity(w1=\"professor\",w2=\"teacher\"))\n",
    "print(wheel_model.wv.similarity(w1=\"professor\",w2=\"friend\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also play with analogy tasks. The commonly seen task is:\n",
    "\n",
    "'Man is to King as Woman is to ____?'\n",
    "\n",
    "The general structure is:\n",
    "`A is to A\\*  as  B is to B\\*`\n",
    "                         \n",
    "gensim provides two different ways of implementing this task. You may be familiar with the the additive version also called the 3CosAdd method:\n",
    "\n",
    "$$\\underset{b*\\in V}{\\textrm{arg max}} (cos(b*,b) - cos(b*,a) + cos(b*,a*))$$\n",
    "\n",
    "This reflects the abstraction of Woman - Man + King. In this maximization, we are searching which word vector will allow us to produce the highest value in this equation.\n",
    "\n",
    "We can implement this method with a built-in function. Positive here refers to words that give the positive contribution to similarity (nominator), and negative refers to words that contribute negatively (denominatory). Here's the additive method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogies\n",
    "# format is: \"man is to professor as woman is to ???\"\n",
    "# note that i'm using \"she\" and \"he\" here since they occur much more frequently in this particular corpus\n",
    "\n",
    "result = wheel_model.wv.most_similar(positive=['she', 'professor'], negative=['he'])\n",
    "\n",
    "print(\"{}: {:.4f}\".format(*result[0])) # this prints the top result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2:\n",
    "\n",
    "**Copy the code above and test out some analogies until you find one that gets you some interesting (or problematic) results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There's so much more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim has quite a few built-in tools, and it's worth taking some time to see what's available. Check the documentation here: [https://radimrehurek.com/gensim/models/keyedvectors.html](https://radimrehurek.com/gensim/models/keyedvectors.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS: Visualization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find below some code you can use to make visualizations from your word2vec model. We can't visualize all the many dimensions in our model, so we need to reduce them to two dimensions for our meager human brains. We do that with something called principal component analysis (PCA). \n",
    "\n",
    "Don't worry about the details for now. This is just a fun way to take a look at the output of our model. \n",
    "\n",
    "**Remember**: Our visualization reduces MANY dimensions to two, so a lot of information is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's do some visualization ###\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get the interactive Tools for Matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.wv.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca_scatterplot(wheel_model, ['exam','stress','finals','rest','sleep'])\n",
    "\n",
    "# display_pca_scatterplot(ccp_model, sample=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3:\n",
    "\n",
    "Play around with the words in the cell above to plot some words that you think might be similar or different from each other. What do you think the plot shows you about the words? Did they confirm or contradict what you though they would show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your answer here "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
