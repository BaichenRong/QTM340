{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf\n",
    "\n",
    "*Lauren F. Klein wrote version 1.0 of this notebook in 2019 based of tutorials by [Matthew Lavin](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf) and [Kavita Ganesan](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XZVlcOdKhSw). Dan Sinykin supplemented it with material from Melanie Walsh's chapter [TF-IDF](https://melaniewalsh.github.io/Intro-Cultural-Analytics/features/Text-Analysis/TF-IDF.html) in 2020. Lauren Klein updated it again in 2021.*\n",
    "\n",
    "We will learn powerful data science techniques soon. But, in many cases, just counting words can tell you a lot. To wit:\n",
    "\n",
    "<img src=\"http://lklein.com/wp-content/uploads/2021/10/Screen-Shot-2021-10-06-at-3.33.34-PM.png\">\n",
    "\n",
    "Today, we're going to explore a method called Term Frequency - Inverse Document Frequency (tf-idf). Tf-idf comes up a lot in text analysis projects because it’s both a corpus exploration method and a pre-processing step for many other text-mining measures and models.\n",
    "\n",
    "The procedure was introduced in a 1972 paper by Karen Spärck Jones under the name “term specificity,” and the basic idea is this:\n",
    "\n",
    "Instead of representing a term in a document by its raw frequency or its relative frequency (the term count divided by the document length), each term is *weighted* by dividing the term frequency by the number of documents in the corpus containing the word. \n",
    "\n",
    "The overall effect of this weighting scheme is to avoid a common problem when conducting text analysis: the most frequently used words in any particular document are often the most frequently used words in all of the documents.\n",
    "\n",
    "By contrast, terms with the highest tf-idf scores are the terms in a document that are distinctively frequent in a document when that document is compared other documents. When you sort by tf-idf score, these distinctive terms rise to the top. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Analogy ##\n",
    "    \n",
    "If this explanation doesn’t quite resonate, a brief analogy might help. \n",
    "\n",
    "Say you've decided leave campus to get dinner on Buford Highway. Since leaving campus takes a lot of effort (and also, crucially, access to a car), the food better be worth it! That means you'll need to balance two competing goals:\n",
    "\n",
    "1) The food has to be really tasty; and also, crucially: \n",
    "2) If you're going to go all the way out to Buford Highway, it better be something that you can't also get in Emory Village. Otherwise, why go to all the trouble of getting there?!\n",
    "\n",
    "Or, to give an example involving actual food: you don't want to go all the way out to Buford Highway to get pizza. Even if the pizza on Buford Highway is pretty tasty, you can get pizza anywhere in town. How can you find out what is distintively tasty on Buford Highway?  \n",
    "\n",
    "If you looked up the Yelp reviews for the all restaurants on Buford highway and sorted by score, you would get an answer to the question of what's the tastiest. But it still won't help solve the problem of what's *distintively tasty* on Buford Highway--like hot pot, for example, which is something that you can't get in Emory Village.   \n",
    "\n",
    "So you need a way to tell the difference between what's tasty and what's distinctively tasty. To do so, you need to distinguish between four categories of food. Food that, on Buford Highway, is:\n",
    "\n",
    "- both tasty and distinctive (e.g. hot pot)\n",
    "- tasty but not distinctive (e.g. pizza) \n",
    "- distinctive but not tasty (e.g. tacos-- tho I'm open to disagreement here)\n",
    "- neither tasty nor distinctive (e.g. Taco Bell--again, open to disagreement).\n",
    "\n",
    "These categories are what tf-idf helps you measure. Term frequencies can be assessed according to the same criteria. A term might be:\n",
    "\n",
    "- Frequently used in the corpus, and used especially frequently in one particular document <-- Interesting! \n",
    "- Frequently used in the corpus, but used frequently in equal measure across all documents <-- Less interesting\n",
    "- Infrequently used in the corpus, but nonetheless used frequently in one particular document <-- Potentially interesting\n",
    "- Infrequently used in in the corpus and also infrequently used in the corpus consitently across all documents <-- Not interesting\n",
    "\n",
    "It's the words that are especially frequent in one document that are most interesting to us, and the ones that TF/IDF helps us identify. To see how, let's take a look at our next corpus--a slightly bigger one--articles published in the *Emory Wheel*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *The Emory Wheel* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we're going to use tf-idf to study the articles published by *The Emory Wheel* betweeen 2014 and 2019. This dataset was created by Honggang Min and Kexin Guan for their final project in the 2019 iteration of this course, and was generously transfered back to me for future class use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing: prepare the documents\n",
    "\n",
    "Tf-idf works on sets of documents. Because we'll be using sk-learn's CountVectorizer, which we learned about last class, in order to count the words, we'll need to get the documents into a list, with each document stored as its own string. \n",
    "\n",
    "In this particular case, the documents are stored in a single CSV file along with some metadata. Below is some code to get the data from the csv format into a list for processing. \n",
    "\n",
    "While this is custom code written for this particular dataset, you'll always need to write some sort of file/text pre-processing code in order to use CountVectorizer and other sk-learn functions. You'll get very familiar with writing code like this by the end of the course! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the file -- using a dataframe since it's already formatted as a csv\n",
    "df = pd.read_csv('../corpora/emory-wheel/sorteddata.csv')\n",
    "\n",
    "# because I happen to know that there are some encoding errors in this corpus,\n",
    "# we'll explicitly convert each string object in this column to unicode\n",
    "df['Content'] = df['Content'].astype('unicode')\n",
    "\n",
    "# and then convert to a list for vectorizing\n",
    "all_docs = df['Content'].tolist()\n",
    "\n",
    "# take a look at the first one\n",
    "print(all_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our sk-learn libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently scikit-learn, which we were introduced to in the previous lesson, allows us to calculate tf-idf with just a few lines of code. We'll go through it slowly first, and then quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a doc-term matrix to calculate tf-idf. Remember how to create a doc-term matrix from last lesson?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer(stop_words='english') # using stopwords this time\n",
    "\n",
    "# this steps generates document-term matrix for the docs\n",
    "dtm=cv.fit_transform(all_docs)\n",
    "\n",
    "# check shape\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many articles do we have in this document-term matrix and how many unique features/terms?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That line, as we learned last clas, tells us that we have 4026 rows, one for each document in the corpus, and 135,198 columns, one for each word (minus single character words, which the tokenizer excludes, as well as the default stopwords, which we've indicated should be excluded with the stop_words='english' parameter above).\n",
    "\n",
    "We can also look at the whole vocabulary like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers above are the indices for each feature, not the word counts. But we can use the indicies in order to generate our word counts. \n",
    "\n",
    "Note that we did a similar thing last class using `cv.get_feature_names()`. This is arguably more efficient and evidently, as of the most recent version of Python, the feature_names method is being depricated, so perhaps better to use this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_words = dtm.sum(axis=0) # sum_words is a vector that contains the number of times each word appears in all \n",
    "                            # the docs in the corpus. In other words, we are summing the elements for each column \n",
    "                            # of the doc-term matrix and storing those counts as a vector \n",
    "\n",
    "# then sort the list of tuples that contain the word and their occurrence in the corpus.\n",
    "# tuples are Python's name for single variables that actually store multiple variables, \n",
    "# like the word and index in the vocabulary attribute above \n",
    "\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()] # rememeber list comprehension! \n",
    "\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# display the top 10\n",
    "words_freq[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already see some words with the most counts don't seem too distinctive: \"emory\" and \"students,\" for example. It's not surprising that those are the most frequently occurring words since the Wheel is a newspaper about Emory students.\n",
    "\n",
    "So now let's calculate the IDF values so that we can balance them out. While we could also calculate these by hand, sk-learn makes it really easy to do it in a few lines of code, so we'll use that instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize TfidfTransformer\n",
    "\n",
    "When you initialize TfidfTransformer, you can choose to set it with different parameters. These parameters will change the way you calculate tf–idf. The recommended way to run `TfidfTransformer` is with smoothing (`smooth_idf = True`) and normalization (`norm='l2'`) turned on. These parameters will better account for differences in document length, and, overall, they'll produce more meaningful tf–idf scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call tfidf_transformer.fit on the word count vector we computed earlier.\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print inverse document frequence (idf) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a dataframe for the idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# sort ascending\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above, the words at the top are those that appear in the most number of documents, across all of the corpus; and the words at the bottom are those that appear in the least number of documents.\n",
    "\n",
    "Once again, it makes sense that words like \"Emory\" and \"said\" are at the top. It's a newspaper, after all! \n",
    "\n",
    "The words at the bottom appear to be either typos or whitespace errors. I'm guessing most of those appear only once across the entire corpus. \n",
    "\n",
    "## IDF by the numbers\n",
    "\n",
    "But what are these numbers that we're looking at?\n",
    "\n",
    "The most direct formula would be **N/df<sub>i</sub>**, where N represents the total number of documents in the corpus, and df is the number of documents in which the term appears. \n",
    "\n",
    "However, many implementations of tf-idf, including scikit-learn, which we are using, normalize the results with additional operations. \n",
    "\n",
    "In tf-idf, normalization is generally used in two ways, and for two reasons: first, to prevent bias in term frequency from terms in shorter or longer documents; and second, as above, to calculate each term’s idf value. \n",
    "\n",
    "Scikit-learn’s implementation of tf-idf represents N as **N+1**, calculates the natural logarithm of **(N+1)/df<sub>i</sub>**, and then adds **1** to the final result. Here is this same thing formatted slightly more nicely:\n",
    "\n",
    "<img src=\"http://lklein.com/wp-content/uploads/2019/10/Screen-Shot-2019-10-02-at-11.52.31-PM.png\">\n",
    "\n",
    "**Important note!** This is only one way to calculate TF-IDF. There are many, many versions. The number itself isn't important. It's the *ranking* that the number enables that's most interesting to us. Because one you have the IDF values, you can now compute the tf-idf scores for any document or set of documents. \n",
    "\n",
    "So now let’s compute tf-idf scores for the documents in our corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce & print tf-idf scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the idf values, you can compute the tf-idf scores for any document or set of documents. Let’s compute tf-idf scores for the documents in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf-idf scores\n",
    "tf_idf_vector=tfidf_transformer.transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s print the tf-idf values of the first document to see if they make sense. \n",
    "\n",
    "We'll place the tf-idf scores from the first document into a pandas dataframe and sort the dataframe in descending order of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "#get tfidf vector for first document\n",
    "first_document_vector=tf_idf_vector[0]\n",
    " \n",
    "#print the scores for the first doc\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that only certain words have scores. This is because only the words in this document have a tf-idf score and everything else, from other documents, shows up as zeroes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf: the fast way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're now tf-idf pros, we're going to use scikit-learn's all-in-one tf-idf vectorizer to do this entire notebook again in two lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(stop_words='english', use_idf=True) # excludings stopwords again\n",
    " \n",
    "# send in all your docs here\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# place tf-idf values for all docs in a pandas dataframe\n",
    "# tfidf_df = pd.DataFrame(tfidf_vectorizer_vectors.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_vectorizer_vectors.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add row for number of times word appears in all documents\n",
    "tfidf_df.loc['Document Frequency'] = (tfidf_df > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at specific words and how they appear in our newspaper corpus. I've entered five words below that we might want to investigate given the recent [symposium on Emory's legacy of slavery](https://libraries.emory.edu/slavery-symposium/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_slice = tfidf_df[['slavery', 'white', 'black', 'history', 'change']]\n",
    "tfidf_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does this output make sense? What does it tell you about which articles you might want to go read? What about some research questions you might ask of this corpus using TF-IDF?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
