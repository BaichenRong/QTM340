{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic Modeling ##\n",
    "\n",
    "*Lauren Klein wrote this lesson in 2019 drawing on writing by [Ted Underwood](https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/) and [Matthew Jockers](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/), this [video of a talk by David Mimno](https://vimeo.com/53080123), and [this notebook](https://radimrehurek.com/topic_modeling_tutorial/2%20-%20Topic%20Modeling.html) by Radim Rehurek. It was supplemented in 2020 with additional materials from Dan Sinykin, and revised again in 2021 by Lauren Klein.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Topic Modeling? ##\n",
    "\n",
    "In both the Li and Bamman paper, and the Antoniak et al. paper, we've seen how topic modeling plays a major role. What is topic modeling? At its most basic level, topic modeling is an automated method for extracting the themes, or \"topics,\" from large sets of documents--like GPT-3 generated fiction, or birth stories, or as we'll explore today, articles in the Emory Wheel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are numerous kinds of topic models, but the most popular and widely-used kind is latent Dirichlet allocation (LDA). It's so popular, in fact, that \"LDA\" and \"topic model\" are sometimes used interchangeably, even though LDA is only one type.\n",
    "\n",
    "LDA math is pretty complicated. We're not going to get very deep into the math just yet (or maybe not ever, depending on the time). But first we are going to introduce two important concepts that will help us conceptually understand how LDA topic models work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1) LDA is an Unsupervised Algorithm \n",
    "Topic modeling is a kind of machine learning. Machine learning always sounds complicated, but it really just means that computer algorithms are performing tasks without being explicitly programmed to do so and that they are \"learning\" how to perform these tasks by being fed training data. In the field of machine learning, algorithms are typically split into two broad categories: supervised and unsupervised. These categories describe how the algorithms are \"trained\" or how they \"learn.\" LDA is an unsupervised algorithm.\n",
    "\n",
    "If an algorithm is supervised, that means a researcher is helping to guide it with some kind of information, like labels. For example, if you wanted to create an algorithm that could identify pictures of cats vs pictures of dogs, you could train it with a bunch of pictures of cats that were clearly labeled CAT and a bunch of pictures of dogs that were clearly labeled DOG. The algorithm would then be able to learn which features are specific to cats vs dogs because you explicitly told it: this is a picture of a cat; this is a picture of a dog.\n",
    "\n",
    "If an algorithm is unsupervised, that means a researcher does not train it with outside information. There are no labels. The algorithm just learns that pictures of cats are more similar to each other and pictures of dogs are more similar to each other. The algorithm doesn't really know that one cluster is cats and one cluster is dogs; it just knows that there are two distinct clusters.\n",
    "\n",
    "Because LDA is an unsupervised algorithm, we don't tell our topic model which words or topics to look for. We only tell the topic model how many topics (or clusters of words) that we want returned. The topic model doesn't know anything about Frida Kahlo, Nella Larsen, and Jackie Robinson. It doesn't know anything about art, literature, and sports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 2) LDA is a Probabilistic Model \n",
    "LDA fundamentally relies on statistics and probabilities. Rather than calculating precise and unchanging metrics about a given corpus, a topic model makes a series of very sophisticated guesses about the corpus. These guesses will change slightly every time we run the topic model. This is important to remember as we analyze, interpret, and make arguments based on our results. All of our results in this lesson will be probabilities, and they'll change slightly every time we re-run the topic model.\n",
    "\n",
    "When we tell the topic model that we want to extract 15 topics from the Emory Wheel, here's what the topic model does:\n",
    "\n",
    "The topic model starts off with a slightly silly, backwards assumption. The topic model assumes that every single one of the 4000-some-odd articles in the corpus was written by someone who exclusively drew their words from 15 mystery topics, or 15 clusters of words. To spin it in a slightly different way with a different medium, the topic model assumes that there was one master artist with 15 different paints on her palette, who created all the articles by dipping her brush into these 15 paints alone, applying and blending them onto each canvas in different proportions. The topic model is trying to discover the 15 mystery topics that created all the Wheel articles, as well as the mixture of these topics that makes up each individual article.\n",
    "\n",
    "The topic model begins by taking a completely wild guess about the 15 topics, but then it iterates through all the words in all the article and makes better and better guesses. If the word \"student\" keeps showing up with the words \"stress\" and \"exam,\" and if all three words keep showing up in the same kinds of article, then the topic model starts to suspect that these three words should belong to the same topic. If the word \"film\" keeps showing up with \"Atlanta\" and \"industry,\" then the topic model suspects that they should belong to the same topic, too. The topic model finally arrives at its best guesses for the 15 topics that most likely created all the Emory Wheel articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LDA explained again in more abstract terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Probabilistic topic models begin with an assumption and a definition. \n",
    "\n",
    "The assumption: all documents contain a mixture of different topics.\n",
    "\n",
    "The definition: a topic is a collection of words, each with a different probability of occurance in a particular document (or other chunk of text) discussing that topic. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's a nice illustration, created by Ted Underwood, that shows this assumed relatioship between topics and documents. \n",
    "\n",
    "![topics and docs](https://tedunderwood.files.wordpress.com/2012/04/shapeart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Above we see an example of the basic assumption of topic modeling: one topic might contain many occurrences of “organize,” “committee,” “direct,” and “lead.” Another might contain a lot of “mercury” and “arsenic,” with a few occurrences of “lead.” \n",
    "\n",
    "The three documents are assumed to contain both topics in different proportions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But here is the thing: we can’t directly observe topics. All we actually have are the documents that attest to their existence. So in other words:\n",
    "\n",
    "**Topic modeling is a way of extrapolating backward from a collection of documents to infer the topics that could have generated them.** \n",
    "\n",
    "There is simply no way to infer the exact topics in a set of documents; there are too many unknowns. So (probabalistic) topic modeling works backwards. It pretends that the problem is mostly solved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How does this play out in actual life?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we knew which topic produced every word in the collection, except for this one word in document D. The word happens to be “lead,” which we’ll call word type W. How are we going to decide whether this occurrence of W belongs to topic 1 or topic 2?\n",
    "\n",
    "![topics and docs](https://tedunderwood.files.wordpress.com/2012/04/shapeart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can’t know for sure. But one way to guess is to consider two questions. This is the first: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* How often does “lead” appear in topic 1 elsewhere? If “lead” often occurs in discussions of 1, then this instance of “lead” might belong to 1 as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But a word can be common in more than one topic, as it is in topics 1 and 2 above. And we don’t want to assign “lead” to a topic about leadership (topic 1) if this document is mostly about heavy metal contamination (topic 2). So we also need to consider a second question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* How common is topic 1 in the rest of the document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To answer these questions, here’s what we’ll do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For each possible topic Z, we’ll multiply the frequency of this word type W in Z by the number of other words in document D that already belong to Z. The result will represent the probability that this word came from Z. Here’s the actual formula:\n",
    "\n",
    "![LDA formula](https://tedunderwood.files.wordpress.com/2012/04/ldaformula.png)\n",
    "\n",
    "There are also a few Greek letters scattered in there, but they aren’t important for our purposes. Technically, they’re called “hyperparameters,” but you can think of them simply as fudge factors. \n",
    "\n",
    "In other words: there’s some chance that this word belongs to topic Z even if it is nowhere else associated with Z; the fudge factors keep that possibility open. (If you want to understand hyperparameters beyond the \"fudge factor\" explanation, see \"[Rethinking LDA: Why Priors Matter](http://people.cs.umass.edu/~mimno/publications.html).\")\n",
    "\n",
    "The overall emphasis on probability in this technique, of course, is why it’s called *probabilistic topic modeling*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Enter Sampling ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now, suppose that instead of having the problem mostly solved, we had only a wild guess which word belonged to which topic. We could still use the strategy I've just described to improve our guess, by making it more internally consistent. \n",
    "\n",
    "We could go through the collection, word by word, and reassign each word to a topic, guided by the formula above. \n",
    "\n",
    "And in fact, that's what LDA actually does.\n",
    "\n",
    "And as we do that, two things happen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1) Words will gradually become more common in topics where they are already common. And also,\n",
    "\n",
    "2) Topics will become more common in documents where they are already common. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Thus our model will gradually become more consistent as topics focus on specific words and documents. But it can’t ever become perfectly consistent, because words and documents don’t line up in one-to-one fashion. So the tendency for topics to concentrate on particular words and documents will eventually be limited by the actual, messy distribution of words across documents.\n",
    "\n",
    "That’s how topic modeling works in practice. You assign words to topics randomly and then just keep improving the model, to make your guess more internally consistent, until the model reaches an equilibrium that is as consistent as the collection allows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For a slightly more in depth explanation of how LDA works, see [this video](https://vimeo.com/53080123). (Start around 5:35). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A brief historical / technical digression... ###\n",
    "\n",
    "Topic modeling began as a US military project in the in the 1990s. The goal was to automatically detect changes in newswire text so that governmental and military organizations could be alerted to emerging geopolitical events. (For more on this history, see [Binder](https://dhdebates.gc.cuny.edu/read/untitled/section/4b276a04-c110-4cba-b93d-4ded8fcfafc9#ch18).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the early 2000s, a team of computer science researchers released [MALLET](http://mallet.cs.umass.edu/topics.php), short for **MA**chine **L**earning for **L**anguag**E** **T**oolkit. As the name suggests, MALLET is a software toolkit that enables a range of NLP techniques. Today, people mostly only use it for topic modeling, which it remains very very good at.\n",
    "\n",
    "With that said, MALLET is written in Java, which means that it's not ideal for working in Python and Jupyter notebooks. None other than Maria Antoniak has written a convenient Python package that allows you to use MALLET in a Jupyter notebook. Her package is called [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), and I'm working on getting it set up for our JupyterHub.\n",
    "\n",
    "Until then, we'll be using [gensim](https://radimrehurek.com/gensim/about.html), a native Python library for topic modeling tht was created in the early 2010s by a computer science PhD student, Radim Rehurek. Its ease of use has made the use of topic models explode--although, I should note, most people end up returning to MALLET for research-level code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's go! ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and setup modules we'll be using in this notebook\n",
    "import logging # for logging status etc\n",
    "import itertools # helpful library for iterating through things\n",
    "\n",
    "import numpy as np # this is a powerful python math package that many others are based on\n",
    "import gensim # our topic modeling library\n",
    "import os # for file i/o\n",
    "\n",
    "# configure logging \n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  \n",
    "\n",
    "# a helpful function that returns the first `n` elements of the stream as plain list.\n",
    "# we'll use this later\n",
    "def head(stream, n=10):\n",
    "    return list(itertools.islice(stream, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some more modules for processing the corpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing ###\n",
    "\n",
    "As previously discussed, many NLP tasks require that you first tokenize your corpus. We've used tokenziers built into both NLTK and spaCy already for this course.  \n",
    "\n",
    "Here, however, we're going to write our own quick tokenizing function that makes use of gensim's [simple_preprocess function](https://radimrehurek.com/gensim/utils.html), which breaks a document into a list of lowercase tokens. The lower-casing is important for topic modeling since we want both uppercase and lowercase versions of the same word to be counted together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's some nice dense python for you:\n",
    "# this defines our tokenize function for future use\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further pre-processing our corpus ###\n",
    "\n",
    "This is the other necessary step before running a topic model. You need to write a function that iterates through your corpus and returns each document in the format (title, tokens). \n",
    "\n",
    "**Side note that we've not yet discussed tuples.** Tuples exist in many programming languages, including R. For our purposes, just know that tuples are sequences of objects--  just like lists-- but they cannot be changed. In Python, you indicate a tuple with parentheses. \n",
    "\n",
    "In any case, the gensim documentation tells us that we want to define a pre-processing function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to yield each doc in a base directory as a `(filename, tokens)` tuple.\n",
    "\n",
    "def iter_docs(base_dir):\n",
    "    docCount = 0\n",
    "    docs = os.listdir(base_dir)\n",
    "\n",
    "    for doc in docs:\n",
    "        if not doc.startswith('.'):\n",
    "            with open(base_dir + doc, \"r\") as file:\n",
    "                text = file.read()\n",
    "                tokens = tokenize(text) \n",
    "        \n",
    "                yield doc, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the stream for later processing \n",
    "stream = iter_docs('../corpora/emory-wheel/articles/')\n",
    "\n",
    "# while we're at it, take a look at what this looks like for the first five docs\n",
    "for doc, tokens in itertools.islice(stream, 5):\n",
    "    print(doc, tokens[:10])  # print the doc title and its first ten tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a Dictionary (not to be confused with a Python dictionary) which maps each word to a numerical ID. \n",
    "\n",
    "This mapping step is required because most algorithms, including gensim's implementation of LDA, rely on numerical libraries that work with vectors indexed by integers, not by strings. Also, many functions need to know the vector/matrix dimensionality in advance.\n",
    "\n",
    "The mapping can be constructed automatically by giving gensim's Dictionary class a stream of tokenized documents, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the stream \n",
    "# this is the one line you'd change here with another corpus and/or corpus location\n",
    "stream = iter_docs('../corpora/emory-wheel/articles/')\n",
    "\n",
    "# all of the rest is standard from the gensim documentation\n",
    "doc_stream = (tokens for _, tokens in stream)\n",
    "              \n",
    "id2word_wheel = gensim.corpora.Dictionary(doc_stream) \n",
    "\n",
    "print(id2word_wheel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dictionary (id2word_wheel) now contains all words that appeared in the corpus, along with how many times they appeared. \n",
    "\n",
    "gensim provides a handy function for mapping tokens to their ID numbers, not unlike the sk-learn vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word_wheel.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't many things you need to do in order to tune your topic model, but one important thing do consider is whether you should filter the words. \n",
    "\n",
    "gensim also provides functions for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line below would, for example, filter out 50 most frequent words\n",
    "# it's commented out here because I don't want to use it in this case\n",
    "# but it's handy to know about \n",
    "# id2word_wheel.filter_n_most_frequent(50)\n",
    "\n",
    "# this line filters out words that appear only 1 doc, keeping the rest\n",
    "# I will use this one \n",
    "# note how no_below and no_above take different values\n",
    "id2word_wheel.filter_extremes(no_below=2, no_above=1.0)\n",
    "\n",
    "id2word_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by removing the words that only appeared in a single document, we went from 118,786 unique words (or tokens) to 43,169. That's not a huge number for a topic model, but we'll see how it goes... \n",
    "\n",
    "Snce a streamed corpus and a dictionary is all we need to create the vectors for our topic model, we can get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a class we need; this is the same for every topic model you create with gensim. \n",
    "# no need to modify it here\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        for title, tokens in itertools.islice(iter_docs(self.dump_file), self.clip_docs):\n",
    "            self.titles.append(title)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stream of bag-of-words vectors\n",
    "wheel_corpus = Corpus('../corpora/emory-wheel/articles/', id2word_wheel)\n",
    "\n",
    "# print the first vector in the stream to see what it looks like; \n",
    "# this is in the format (word_id, count in first doc)\n",
    "\n",
    "vector = next(iter(wheel_corpus))\n",
    "\n",
    "vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we're ready to run our topic model!\n",
    "\n",
    "%time lda_model = gensim.models.LdaModel(wheel_corpus, num_topics=15, id2word=id2word_wheel, passes=5) \n",
    "\n",
    "# note that passes should be higher -- usually in the 50-100 range -- \n",
    "# but in the interests of time we'll only do 5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some additional helpful functions built into LdaModel\n",
    "\n",
    "# how to store corpus to disk\n",
    "from gensim.corpora import MmCorpus\n",
    "MmCorpus.serialize('./wheel.corpus.mm', wheel_corpus)\n",
    "\n",
    "# how to store dictionary to disk\n",
    "id2word_wheel.save('./wheel.dictionary')\n",
    "\n",
    "# how to store model to disk \n",
    "lda_model.save('./lda_wheel-15topics_5iters.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load in a saved model. This is very helpful to know about, since generating new topic models takes time. \n",
    "\n",
    "Here, we're going to load in a (slightly) better topic model of the Emory Wheel with the same number of topics (15), but 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an old model; in this case, a topic model of the ccp with 50 iterations\n",
    "lda_model = gensim.models.LdaModel.load('./lda_wheel-15topics_50iters.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim comes with a bunch of functions that make interacting with the output of the topic\n",
    "# model a little easier. this one shows the topics. \n",
    "\n",
    "# show the topics, in the format (number of topics to show, number of terms)\n",
    "# note that all words are in all topics, just some topics consist of very very small\n",
    "# proportions of that word\n",
    "\n",
    "# as you can tell already, even the top words in each topic are only a very small proportion\n",
    "# of that topic, since we are dealing with about 14K unique words\n",
    "\n",
    "lda_model.show_topics(15, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's format the words a little more nicely; \n",
    "# the formatted=False parameter returns tuples of (word, probability)\n",
    "\n",
    "topics = lda_model.show_topics(15, 20, formatted=False)\n",
    "\n",
    "for topic in topics:\n",
    "    topic_num = topic[0]\n",
    "    topic_words = \"\"\n",
    "    \n",
    "    topic_pairs = topic[1]\n",
    "    for pair in topic_pairs:\n",
    "        topic_words += pair[0] + \", \"\n",
    "    \n",
    "    print(\"T\" + str(topic_num) + \": \" + topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics and Labels\n",
    "\n",
    "Now you can see, perhaps, that we call a \"topic\" is really just a list of the most probable words for that topic, which are sorted in descending order of probability. The most probable word for the topic is the first word. \n",
    "\n",
    "Topic models start to get more powerful when we, as human researchers, analyze the most probable words for every topic and summarize what these words have in common. This summary can then be used as a descriptive label for the topic. Remember, since an LDA topic model is an unsupervised algorithm, it doesn't know what these words mean in relationship to one another. It's up to us, as the human researchers, to make meaning out of the topics.\n",
    "\n",
    "How might you label the following topics?\n",
    "\n",
    "✨Topic 3:✨\n",
    "\n",
    "`team, emory, game, eagles, said, university, season, second, time, points, senior, win, junior, year, sophomore, teams, freshman, college, run, play, `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✨Topic 6:✨\n",
    "\n",
    "`killers, song, album, music, songs, rae, band, stage, concert, performance, crowd, audience, artists, lyrics, sound, dance, makeshift, collapsing, nextyear, weighty, `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✨Topic 13:✨\n",
    "\n",
    "`officer, subject, said, april, epd, reported, complainant, emory, case, individual, student, wallet, responded, assigned, investigator, number, second, stage, campus, driver, `\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining the topic model\n",
    "\n",
    "These are decent topics, but they're not amazing. Here are a few things you might want to try in order to fine-tune your model:\n",
    "\n",
    "* Filtering some of the most common words (see the filtering function above)\n",
    "* Generating fewer topics (we could try 10, for instance). \n",
    "\n",
    "Feel free to try those things on your own. \n",
    "\n",
    "But for the purposes of this class, let's take a bit of a closer look at the probabilities attached to each word in a single topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T13 looks coherent\n",
    "topic = topics[13]\n",
    "\n",
    "# the first item of the topic is the topic number\n",
    "topic_num = topic[0]\n",
    "\n",
    "# the next item is another list with pairs of words and percentages\n",
    "topic_pairs = topic[1]\n",
    "for pair in topic_pairs:\n",
    "    print(pair[0] + \": \" + str(pair[1]))\n",
    "\n",
    "# since all topics contain all words, the sum of all of the probabilities of each \n",
    "# topic should be 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's flip it around and look at the topical composition of a single document. \n",
    "\n",
    "NB: MALLET provides this output automatically, but with gensim there's a bit more work required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] \n",
    "\n",
    "# open one file\n",
    "with open('../corpora/emory-wheel/articles/2014-10-02-Atlanta-Food-Truck-Park-Brings-Enriching-Epicurian-Experience.txt', \"r\") as file:\n",
    "    text = file.read()\n",
    "    tokens = tokenize(text) # remember this from above\n",
    "\n",
    "# create the bag of words for the document on the basis of the Wheel dictionary, created above\n",
    "doc_bow = id2word_wheel.doc2bow(tokens)\n",
    "\n",
    "# get the topics that the doc consists of\n",
    "doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "\n",
    "doc_topics\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can cross-reference to find those topics and words\n",
    "\n",
    "for topic, prob in doc_topics:\n",
    "    print(\"T\" + str(topic) + \": \" + \"{:.2%}\".format(prob) + \" of document.\")\n",
    "          \n",
    "        #  str(round(prob, 2)))\n",
    "\n",
    "    topic_words = \"Top words in topic: \"\n",
    "    select_topics = topics[topic]\n",
    "    \n",
    "    for pair in select_topics[1]:\n",
    "        topic_words += pair[0] + \", \"\n",
    "    \n",
    "    print(topic_words)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Topics ###\n",
    "\n",
    "Gensim has several built-in methods for evaluating topics included as a model called [CoherenceModel](https://radimrehurek.com/gensim/models/coherencemodel.html). The fastest one to calculate is called u_mass, and in this case, the closer to zero (positive or negative), the better the score. \n",
    "\n",
    "Let's see how our model performs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "cm = CoherenceModel(model=lda_model, corpus=wheel_corpus, coherence='u_mass')\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "\n",
    "coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a review essay by Hanna Wallach et al. that summarizes a few methods of evaluation, including some involving humans in the loop: [\"Evaluation Methods for Topc Models\"](http://dirichlet.net/pdf/wallach09evaluation.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to evalute topics is just to look at them.\n",
    "\n",
    "The pyLDAvis library lets you do this in a single line. It's very satisfying! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA visualization tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "\n",
    "# just reformat the corpus for pyLDAvis \n",
    "from gensim.corpora import MmCorpus\n",
    "wheel_mm_corpus = MmCorpus('./wheel.corpus.mm')\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.gensim.prepare(lda_model, wheel_mm_corpus, id2word_wheel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
