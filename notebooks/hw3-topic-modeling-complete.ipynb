{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic Modeling Homework ##\n",
    "\n",
    "*The text in the first half of this notebook restates much of what was covered in Thursday's lecture. Please read through it again since, as I explained in class, topic modeling is a heady concept that can take a while to settle in your brain.*\n",
    "\n",
    "*Alternately, or in addition, you may wish to watch [this video](https://vimeo.com/53080123) for a third (!) explanation of how topic modeling works. (Start around 3:30).* \n",
    "\n",
    "*If, in the end, topic modeling still seems confusing to you, that is also fine. It may make more sense after you've had a chance to play around with the output of an actual model--which is what the second half of this notebook allows you to do.*\n",
    "\n",
    "*There are three exercises embedded in this notebook, all located towards the end. Please complete the exercises and when you're done, upload your completed notebook to Canvas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Topic Modeling Explained ##\n",
    "\n",
    "*Per above, as the first part of your homework, please just read through these next few sections.*\n",
    "\n",
    "### What is Topic Modeling? ###\n",
    "\n",
    "In both the Li and Bamman paper, and the Antoniak et al. paper, we've seen how topic modeling plays a major role. What is topic modeling? At its most basic level, topic modeling is an automated method for extracting the themes, or \"topics,\" from large sets of documents--like GPT-3 generated fiction, or birth stories, or as we'll explore today, articles in the Emory Wheel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are numerous kinds of topic models, but the most popular and widely-used kind is latent Dirichlet allocation (LDA). It's so popular, in fact, that \"LDA\" and \"topic model\" are sometimes used interchangeably, even though LDA is only one type.\n",
    "\n",
    "LDA math is pretty complicated. We're not going to get very deep into the math just yet (or maybe not ever, depending on the time). But first we are going to introduce two important concepts that will help us conceptually understand how LDA topic models work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1) LDA is an Unsupervised Algorithm \n",
    "Topic modeling is a kind of machine learning. Machine learning always sounds complicated, but it really just means that computer algorithms are performing tasks without being explicitly programmed to do so and that they are \"learning\" how to perform these tasks by being fed training data. In the field of machine learning, algorithms are typically split into two broad categories: supervised and unsupervised. These categories describe how the algorithms are \"trained\" or how they \"learn.\" LDA is an unsupervised algorithm.\n",
    "\n",
    "If an algorithm is supervised, that means a researcher is helping to guide it with some kind of information, like labels. For example, if you wanted to create an algorithm that could identify pictures of cats vs pictures of dogs, you could train it with a bunch of pictures of cats that were clearly labeled CAT and a bunch of pictures of dogs that were clearly labeled DOG. The algorithm would then be able to learn which features are specific to cats vs dogs because you explicitly told it: this is a picture of a cat; this is a picture of a dog.\n",
    "\n",
    "If an algorithm is unsupervised, that means a researcher does not train it with outside information. There are no labels. The algorithm just learns that pictures of cats are more similar to each other and pictures of dogs are more similar to each other. The algorithm doesn't really know that one cluster is cats and one cluster is dogs; it just knows that there are two distinct clusters.\n",
    "\n",
    "Because LDA is an unsupervised algorithm, we don't tell our topic model which words or topics to look for. We only tell the topic model how many topics (or clusters of words) that we want returned. The topic model doesn't know anything about Frida Kahlo, Nella Larsen, and Jackie Robinson. It doesn't know anything about art, literature, and sports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### 2) LDA is a Probabilistic Model \n",
    "LDA fundamentally relies on statistics and probabilities. Rather than calculating precise and unchanging metrics about a given corpus, a topic model makes a series of very sophisticated guesses about the corpus. These guesses will change slightly every time we run the topic model. This is important to remember as we analyze, interpret, and make arguments based on our results. All of our results in this lesson will be probabilities, and they'll change slightly every time we re-run the topic model.\n",
    "\n",
    "When we tell the topic model that we want to extract 15 topics from the Emory Wheel, here's what the topic model does:\n",
    "\n",
    "The topic model starts off with a slightly silly, backwards assumption. The topic model assumes that every single one of the 4000-some-odd articles in the corpus was written by someone who exclusively drew their words from 15 mystery topics, or 15 clusters of words. To spin it in a slightly different way with a different medium, the topic model assumes that there was one master artist with 15 different paints on her palette, who created all the articles by dipping her brush into these 15 paints alone, applying and blending them onto each canvas in different proportions. The topic model is trying to discover the 15 mystery topics that created all the Wheel articles, as well as the mixture of these topics that makes up each individual article.\n",
    "\n",
    "The topic model begins by taking a completely wild guess about the 15 topics, but then it iterates through all the words in all the article and makes better and better guesses. If the word \"student\" keeps showing up with the words \"stress\" and \"exam,\" and if all three words keep showing up in the same kinds of article, then the topic model starts to suspect that these three words should belong to the same topic. If the word \"film\" keeps showing up with \"Atlanta\" and \"industry,\" then the topic model suspects that they should belong to the same topic, too. The topic model finally arrives at its best guesses for the 15 topics that most likely created all the Emory Wheel articles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LDA explained again in more concrete terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Probabilistic topic models begin with an assumption and a definition. \n",
    "\n",
    "The assumption: all documents contain a mixture of different topics.\n",
    "\n",
    "The definition: a topic is a collection of words, each with a different probability of occurance in a particular document (or other chunk of text) discussing that topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's a nice illustration, created by Ted Underwood, that shows this assumed relatioship between topics and documents. \n",
    "\n",
    "![topics and docs](https://tedunderwood.files.wordpress.com/2012/04/shapeart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Above we see an example of the basic assumption of topic modeling: one topic might contain many occurrences of “organize,” “committee,” “direct,” and “lead.” Another might contain a lot of “mercury” and “arsenic,” with a few occurrences of “lead.” \n",
    "\n",
    "The three documents are assumed to contain both topics in different proportions.\n",
    "\n",
    "But here is the thing: we can’t directly observe topics. All we actually have are the documents that attest to their existence. So in other words:\n",
    "\n",
    "**Topic modeling is a way of extrapolating backward from a collection of documents to infer the topics that could have generated them.** \n",
    "\n",
    "There is simply no way to infer the exact topics in a set of documents; there are too many unknowns. So (probabalistic) topic modeling works backwards. It pretends that the problem is mostly solved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How does this play out in actual life?**\n",
    "\n",
    "Suppose we knew which topic produced every word in the collection, except for this one word in document D. The word happens to be “lead,” which we’ll call word type W. How are we going to decide whether this occurrence of W belongs to topic 1 or topic 2?\n",
    "\n",
    "![topics and docs](https://tedunderwood.files.wordpress.com/2012/04/shapeart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can’t know for sure. But one way to guess is to consider two questions. This is the first: \n",
    "\n",
    "* How often does “lead” appear in topic 1 elsewhere? If “lead” often occurs in discussions of 1, then this instance of “lead” might belong to 1 as well. \n",
    "\n",
    "But a word can be common in more than one topic, as it is in topics 1 and 2 above. And we don’t want to assign “lead” to a topic about leadership (topic 1) if this document is mostly about heavy metal contamination (topic 2). So we also need to consider a second question:\n",
    "\n",
    "* How common is topic 1 in the rest of the document?\n",
    "\n",
    "To answer these questions, here’s what we’ll do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For each possible topic Z, we’ll multiply the frequency of this word type W in Z by the number of other words in document D that already belong to Z. The result will represent the probability that this word came from Z. Here’s the actual formula:\n",
    "\n",
    "![LDA formula](https://tedunderwood.files.wordpress.com/2012/04/ldaformula.png)\n",
    "\n",
    "There are also a few Greek letters scattered in there, but they aren’t important for our purposes. Technically, they’re called “hyperparameters,” but you can think of them simply as fudge factors. \n",
    "\n",
    "In other words: there’s some chance that this word belongs to topic Z even if it is nowhere else associated with Z; the fudge factors keep that possibility open. (If you want to understand hyperparameters beyond the \"fudge factor\" explanation, see \"[Rethinking LDA: Why Priors Matter](http://people.cs.umass.edu/~mimno/publications.html).\")\n",
    "\n",
    "The overall emphasis on probability in this technique, of course, is why it’s called *probabilistic topic modeling*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Enter Sampling**\n",
    "\n",
    "Now, suppose that instead of having the problem mostly solved, we had only a wild guess which word belonged to which topic. We could still use the strategy I've just described to improve our guess, by making it more internally consistent. \n",
    "\n",
    "We could go through the collection, word by word, and reassign each word to a topic, guided by the formula above. \n",
    "\n",
    "And in fact, that's what LDA actually does.\n",
    "\n",
    "And as we do that, two things happen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1) Words will gradually become more common in topics where they are already common. And also,\n",
    "\n",
    "2) Topics will become more common in documents where they are already common. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Thus our model will gradually become more consistent as topics focus on specific words and documents. But it can’t ever become perfectly consistent, because words and documents don’t line up in one-to-one fashion. So the tendency for topics to concentrate on particular words and documents will eventually be limited by the actual, messy distribution of words across documents.\n",
    "\n",
    "That’s how topic modeling works in practice. You assign words to topics randomly and then just keep improving the model, to make your guess more internally consistent, until the model reaches an equilibrium that is as consistent as the collection allows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A brief historical / technical digression... ##\n",
    "\n",
    "Topic modeling began as a US military project in the in the 1990s. The goal was to automatically detect changes in newswire text so that governmental and military organizations could be alerted to emerging geopolitical events. (For more on this history, see [Binder](https://dhdebates.gc.cuny.edu/read/untitled/section/4b276a04-c110-4cba-b93d-4ded8fcfafc9#ch18).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the early 2000s, a team of computer science researchers released [MALLET](http://mallet.cs.umass.edu/topics.php), short for **MA**chine **L**earning for **L**anguag**E** **T**oolkit. As the name suggests, MALLET is a software toolkit that enables a range of NLP techniques. Today, people mostly only use it for topic modeling, which it remains very very good at.\n",
    "\n",
    "With that said, MALLET is written in Java, which means that it's not ideal for working in Python and Jupyter notebooks. None other than Maria Antoniak has written a convenient Python package that allows you to use MALLET in a Jupyter notebook. Her package is called [Little MALLET Wrapper](https://github.com/maria-antoniak/little-mallet-wrapper), and I'm working on getting it set up for our JupyterHub.\n",
    "\n",
    "Until then, we'll be using [gensim](https://radimrehurek.com/gensim/about.html), a native Python library for topic modeling (among other tasks) that was created in the early 2010s by a computer science PhD student, Radim Rehurek. While it's more convenient than MALLET, the topics it generates are generally considered to be less good/coherent than those generated by MALLET, so most people end up returning to MALLET for research-level code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part II: Topic Modeling with Gensim \n",
    "\n",
    "The code to generate topic models with gensim is definitely a little complicated. When we're able to use MALLET, you'll appreciate it for how much more clear it is to set up and query the model. \n",
    "\n",
    "How I recommend you approach the code below is simply to run each cell one-by-one until you get the output of the model. (I'll note where that is). Please don't concern yourself with the code that does the setup and processing, except to a) run it; and b) note that we're seeing yet another version of the standard load libraries / pre-process text / tokenize pipeline. \n",
    "\n",
    "### Load libraries and configure logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and setup modules we'll be using in this notebook\n",
    "import logging # for logging status etc\n",
    "import itertools # helpful library for iterating through things\n",
    "\n",
    "import numpy as np # this is a powerful python math package that many others are based on\n",
    "import gensim # our topic modeling library\n",
    "import os # for file i/o\n",
    "\n",
    "# configure logging since topic modeling can take some time and it's nice to get status updates\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  \n",
    "\n",
    "# a helpful function that returns the first `n` elements of the stream as plain list.\n",
    "# we'll use this later but we'll declare it now\n",
    "def head(stream, n=10):\n",
    "    return list(itertools.islice(stream, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some more modules for processing the corpus\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for pre-processing and tokenizing our text ###\n",
    "\n",
    "As previously discussed, many NLP tasks require that you tokenize your corpus. We've used tokenziers built into both NLTK and spaCy already for this course.  \n",
    "\n",
    "Here, however, we're going to write our own quick tokenizing function that makes use of gensim's [simple_preprocess function](https://radimrehurek.com/gensim/utils.html), which breaks a document into a list of lowercase tokens. The lower-casing is important for topic modeling since we want both uppercase and lowercase versions of the same word to be counted together. \n",
    "\n",
    "We'll define the tokenizing function first, and then use it in our pre-processing function (the second function defined in the cell below). \n",
    "\n",
    "For topic modeling with gensim, we need to pre-process our documents so that they end up in the format (filename, tokens). We're using this format because that's what the gensim documentation tell us to use. In fact, both of these functions come nearly verbatim from the gensim documentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this defines our tokenize function for future use\n",
    "def tokenize(text):\n",
    "    return [token for token in simple_preprocess(text) if token not in STOPWORDS] # more list comprehension syntax! \n",
    "\n",
    "# this define a function that yields each doc in a base directory as a `(filename, tokens)` tuple.\n",
    "def iter_docs(base_dir):\n",
    "    docCount = 0\n",
    "    docs = os.listdir(base_dir)\n",
    "\n",
    "    for doc in docs:\n",
    "        if not doc.startswith('.'):\n",
    "            with open(base_dir + doc, \"r\") as file:\n",
    "                text = file.read()\n",
    "                tokens = tokenize(text) \n",
    "        \n",
    "                yield doc, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the id2word dictionary\n",
    "\n",
    "The next step in generating a topic model with gensim is to create a dictionary (not to be confused with a Python dictionary) which maps each word to a numerical ID. \n",
    "\n",
    "This mapping step is required because almost all models involving text, including this one, work with vectors indexed by integers, not by strings. Also, many functions need to know the vector/matrix dimensionality in advance.\n",
    "\n",
    "The mapping can be constructed automatically by giving gensim's Dictionary class a so-called \"stream\" of tokenized documents, as in the cells below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-06-White-Supremacist-Symbol-Tarnishes-Old-DeKalb-Courthouse.txt ['ve', 'set', 'foot', 'decatur', 'square', 'home', 'jeni', 'splendid', 'ice', 'creams']\n",
      "2017-09-13-Let-the-Games-Begin-Taylor-Swift-Satirizes-her-Reputation-in-NewSingles.txt ['taylor', 'swift', 'knows', 'play', 'media', 'like', 'fiddle', 'real', 'fiddle', 'course']\n",
      "2014-11-11-Crime-Report-11-11-14.txt ['nov', 'emory', 'police', 'department', 'epd', 'responded', 'women', 'restroom', 'floor', 'dobbs']\n",
      "2017-10-18-Doolino-Knows-Best-Falling-Apart.txt ['dear', 'doolino', 'weeks', 'laundry', 've', 'wearing', 'free', 'shirt', 'got', 'fromwonderful']\n",
      "2016-11-23-Former-U-S-Poet-Laureate-to-Leave-Emory-for-Northwestern.txt ['poet', 'laureate', 'natasha', 'trethewey', 'leave', 'emory', 'creative', 'writing', 'program', 'years']\n"
     ]
    }
   ],
   "source": [
    "# set up the document \"stream\" for use below \n",
    "## NOTE PATH MAY NEED TO CHANGE DEPENDING ON THE RELATIVE LOCATION OF YOUR CORPUS\n",
    "stream = iter_docs('../corpora/emory-wheel/articles/') \n",
    "\n",
    "# take a look at what the \"stream\" looks like for the first five docs\n",
    "for doc, tokens in itertools.islice(stream, 5):\n",
    "    print(doc, tokens[:10])  # print the doc title and its first ten tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll have gensim construct the id2word dictionary on the basis of the stream we've set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "INFO : built Dictionary(118785 unique tokens: ['acute', 'adorn', 'afterthe', 'allow', 'amend']...) from 4009 documents (total 1459806 corpus positions)\n",
      "INFO : Dictionary lifecycle event {'msg': \"built Dictionary(118785 unique tokens: ['acute', 'adorn', 'afterthe', 'allow', 'amend']...) from 4009 documents (total 1459806 corpus positions)\", 'datetime': '2021-10-18T09:51:25.238974', 'gensim': '4.1.2', 'python': '3.7.2 (default, Dec 29 2018, 00:00:04) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7fc0890f6550>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code to create id2word dictionary from the gensim documentation\n",
    "\n",
    "# set up the stream (again)\n",
    "stream = iter_docs('../corpora/emory-wheel/articles/') \n",
    "\n",
    "# get the tokens from the stream\n",
    "doc_stream = (tokens for _, tokens in stream)\n",
    "              \n",
    "# send to the dictionary constructor \n",
    "id2word_wheel = gensim.corpora.Dictionary(doc_stream) \n",
    "\n",
    "# print out a sign that it's done\n",
    "id2word_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping tokens to ID numners\n",
    "\n",
    "The gensim dictionary (id2word_wheel, which I've named for the Emory Wheel data it contains) now contains all words that appeared in the corpus, along with how many times they appeared. \n",
    "\n",
    "gensim provides a handy function for mapping tokens to their ID numbers, not unlike the sk-learn vectorizer. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acute': 0,\n",
       " 'adorn': 1,\n",
       " 'afterthe': 2,\n",
       " 'allow': 3,\n",
       " 'amend': 4,\n",
       " 'america': 5,\n",
       " 'amonument': 6,\n",
       " 'andserved': 7,\n",
       " 'argument': 8,\n",
       " 'atlanta': 9,\n",
       " 'board': 10,\n",
       " 'boris': 11,\n",
       " 'building': 12,\n",
       " 'called': 13,\n",
       " 'carefully': 14,\n",
       " 'cause': 15,\n",
       " 'causemonument': 16,\n",
       " 'celebrate': 17,\n",
       " 'children': 18,\n",
       " 'civil': 19,\n",
       " 'commemorates': 20,\n",
       " 'committed': 21,\n",
       " 'composed': 22,\n",
       " 'concerns': 23,\n",
       " 'confederacy': 24,\n",
       " 'confederate': 25,\n",
       " 'conservative': 26,\n",
       " 'conservatives': 27,\n",
       " 'consider': 28,\n",
       " 'construction': 29,\n",
       " 'context': 30,\n",
       " 'contrast': 31,\n",
       " 'core': 32,\n",
       " 'county': 33,\n",
       " 'courthouse': 34,\n",
       " 'covenant': 35,\n",
       " 'creams': 36,\n",
       " 'credence': 37,\n",
       " 'crow': 38,\n",
       " 'current': 39,\n",
       " 'day': 40,\n",
       " 'deadly': 41,\n",
       " 'decades': 42,\n",
       " 'decatur': 43,\n",
       " 'dedicated': 44,\n",
       " 'defend': 45,\n",
       " 'defended': 46,\n",
       " 'dekalb': 47,\n",
       " 'despite': 48,\n",
       " 'directly': 49,\n",
       " 'draw': 50,\n",
       " 'editorial': 51,\n",
       " 'end': 52,\n",
       " 'era': 53,\n",
       " 'erase': 54,\n",
       " 'erected': 55,\n",
       " 'evidence': 56,\n",
       " 'example': 57,\n",
       " 'exclusively': 58,\n",
       " 'fact': 59,\n",
       " 'faith': 60,\n",
       " 'fallen': 61,\n",
       " 'fast': 62,\n",
       " 'fathers': 63,\n",
       " 'foot': 64,\n",
       " 'foroppression': 65,\n",
       " 'founders': 66,\n",
       " 'gazebo': 67,\n",
       " 'george': 68,\n",
       " 'georgia': 69,\n",
       " 'given': 70,\n",
       " 'great': 71,\n",
       " 'guard': 72,\n",
       " 'height': 73,\n",
       " 'held': 74,\n",
       " 'history': 75,\n",
       " 'holds': 76,\n",
       " 'home': 77,\n",
       " 'iberian': 78,\n",
       " 'ice': 79,\n",
       " 'ideals': 80,\n",
       " 'ideology': 81,\n",
       " 'idolization': 82,\n",
       " 'importantly': 83,\n",
       " 'independence': 84,\n",
       " 'individual': 85,\n",
       " 'inequality': 86,\n",
       " 'institution': 87,\n",
       " 'intent': 88,\n",
       " 'inthe': 89,\n",
       " 'jefferson': 90,\n",
       " 'jeni': 91,\n",
       " 'jennifer': 92,\n",
       " 'jim': 93,\n",
       " 'justicesystem': 94,\n",
       " 'katz': 95,\n",
       " 'keeping': 96,\n",
       " 'law': 97,\n",
       " 'lead': 98,\n",
       " 'leaders': 99,\n",
       " 'legacy': 100,\n",
       " 'like': 101,\n",
       " 'line': 102,\n",
       " 'lost': 103,\n",
       " 'lutwyche': 104,\n",
       " 'madeline': 105,\n",
       " 'memorialized': 106,\n",
       " 'men': 107,\n",
       " 'monument': 108,\n",
       " 'monuments': 109,\n",
       " 'museum': 110,\n",
       " 'niyonzima': 111,\n",
       " 'obelisk': 112,\n",
       " 'ofthe': 113,\n",
       " 'old': 114,\n",
       " 'oppression': 115,\n",
       " 'ourcountry': 116,\n",
       " 'parts': 117,\n",
       " 'past': 118,\n",
       " 'phenomenon': 119,\n",
       " 'pig': 120,\n",
       " 'plagued': 121,\n",
       " 'point': 122,\n",
       " 'politicians': 123,\n",
       " 'prohibits': 124,\n",
       " 'public': 125,\n",
       " 'race': 126,\n",
       " 'racism': 127,\n",
       " 'racist': 128,\n",
       " 'rallying': 129,\n",
       " 'reassess': 130,\n",
       " 'reflect': 131,\n",
       " 'relocate': 132,\n",
       " 'relocation': 133,\n",
       " 'remembered': 134,\n",
       " 'remove': 135,\n",
       " 'republic': 136,\n",
       " 'riddled': 137,\n",
       " 'riots': 138,\n",
       " 'set': 139,\n",
       " 'shouldwe': 140,\n",
       " 'slaveowner': 141,\n",
       " 'slavery': 142,\n",
       " 'slippery': 143,\n",
       " 'slope': 144,\n",
       " 'soldiers': 145,\n",
       " 'spaces': 146,\n",
       " 'spirit': 147,\n",
       " 'splendid': 148,\n",
       " 'square': 149,\n",
       " 'standing': 150,\n",
       " 'stands': 151,\n",
       " 'state': 152,\n",
       " 'states': 153,\n",
       " 'statues': 154,\n",
       " 'stood': 155,\n",
       " 'supremacist': 156,\n",
       " 'supremacy': 157,\n",
       " 'symbol': 158,\n",
       " 'symbolizes': 159,\n",
       " 'symbols': 160,\n",
       " 'thedeclaration': 161,\n",
       " 'thedemonization': 162,\n",
       " 'themonument': 163,\n",
       " 'theremoval': 164,\n",
       " 'thomas': 165,\n",
       " 'touristy': 166,\n",
       " 'try': 167,\n",
       " 'unable': 168,\n",
       " 'united': 169,\n",
       " 've': 170,\n",
       " 'venerates': 171,\n",
       " 'view': 172,\n",
       " 'views': 173,\n",
       " 'voice': 174,\n",
       " 'war': 175,\n",
       " 'washington': 176,\n",
       " 'white': 177,\n",
       " 'women': 178,\n",
       " 'writer': 179,\n",
       " 'years': 180,\n",
       " 'abandoned': 181,\n",
       " 'ability': 182,\n",
       " 'absorption': 183,\n",
       " 'accomplishments': 184,\n",
       " 'accused': 185,\n",
       " 'achieve': 186,\n",
       " 'actually': 187,\n",
       " 'acutely': 188,\n",
       " 'addresses': 189,\n",
       " 'ages': 190,\n",
       " 'album': 191,\n",
       " 'alsogets': 192,\n",
       " 'amid': 193,\n",
       " 'andsixth': 194,\n",
       " 'antonoff': 195,\n",
       " 'appearing': 196,\n",
       " 'aug': 197,\n",
       " 'aware': 198,\n",
       " 'backstory': 199,\n",
       " 'bad': 200,\n",
       " 'band': 201,\n",
       " 'bass': 202,\n",
       " 'beats': 203,\n",
       " 'beenher': 204,\n",
       " 'beginning': 205,\n",
       " 'blame': 206,\n",
       " 'blank': 207,\n",
       " 'blood': 208,\n",
       " 'boldreinvention': 209,\n",
       " 'bothworshipped': 210,\n",
       " 'brings': 211,\n",
       " 'buried': 212,\n",
       " 'burton': 213,\n",
       " 'calls': 214,\n",
       " 'career': 215,\n",
       " 'casual': 216,\n",
       " 'chorus': 217,\n",
       " 'cinematic': 218,\n",
       " 'clear': 219,\n",
       " 'collaborator': 220,\n",
       " 'collaborators': 221,\n",
       " 'coming': 222,\n",
       " 'commitment': 223,\n",
       " 'confident': 224,\n",
       " 'constant': 225,\n",
       " 'content': 226,\n",
       " 'country': 227,\n",
       " 'couple': 228,\n",
       " 'course': 229,\n",
       " 'cracks': 230,\n",
       " 'credited': 231,\n",
       " 'criticism': 232,\n",
       " 'criticized': 233,\n",
       " 'critics': 234,\n",
       " 'croons': 235,\n",
       " 'culturally': 236,\n",
       " 'culture': 237,\n",
       " 'dater': 238,\n",
       " 'dead': 239,\n",
       " 'death': 240,\n",
       " 'declared': 241,\n",
       " 'delivery': 242,\n",
       " 'departure': 243,\n",
       " 'descending': 244,\n",
       " 'different': 245,\n",
       " 'divorced': 246,\n",
       " 'door': 247,\n",
       " 'downsynth': 248,\n",
       " 'dress': 249,\n",
       " 'dropped': 250,\n",
       " 'drops': 251,\n",
       " 'easier': 252,\n",
       " 'easily': 253,\n",
       " 'effect': 254,\n",
       " 'elizabeth': 255,\n",
       " 'embodying': 256,\n",
       " 'entire': 257,\n",
       " 'especially': 258,\n",
       " 'ethos': 259,\n",
       " 'ex': 260,\n",
       " 'exactly': 261,\n",
       " 'experimentation': 262,\n",
       " 'extension': 263,\n",
       " 'fairbrass': 264,\n",
       " 'fan': 265,\n",
       " 'fanbase': 266,\n",
       " 'fans': 267,\n",
       " 'feature': 268,\n",
       " 'fiddle': 269,\n",
       " 'fifth': 270,\n",
       " 'fixation': 271,\n",
       " 'flippancy': 272,\n",
       " 'follow': 273,\n",
       " 'fool': 274,\n",
       " 'forget': 275,\n",
       " 'forthcoming': 276,\n",
       " 'fred': 277,\n",
       " 'fun': 278,\n",
       " 'games': 279,\n",
       " 'genre': 280,\n",
       " 'gossip': 281,\n",
       " 'greatest': 282,\n",
       " 'haters': 283,\n",
       " 'herpop': 284,\n",
       " 'hit': 285,\n",
       " 'iconoclastic': 286,\n",
       " 'identity': 287,\n",
       " 'include': 288,\n",
       " 'indication': 289,\n",
       " 'industry': 290,\n",
       " 'infamous': 291,\n",
       " 'infuses': 292,\n",
       " 'innovative': 293,\n",
       " 'instrument': 294,\n",
       " 'intends': 295,\n",
       " 'interpolated': 296,\n",
       " 'inverse': 297,\n",
       " 'jab': 298,\n",
       " 'jack': 299,\n",
       " 'jarring': 300,\n",
       " 'joke': 301,\n",
       " 'kendrick': 302,\n",
       " 'key': 303,\n",
       " 'know': 304,\n",
       " 'knowledge': 305,\n",
       " 'known': 306,\n",
       " 'knows': 307,\n",
       " 'lacksuch': 308,\n",
       " 'lamar': 309,\n",
       " 'leaning': 310,\n",
       " 'lend': 311,\n",
       " 'life': 312,\n",
       " 'list': 313,\n",
       " 'listeners': 314,\n",
       " 'listens': 315,\n",
       " 'little': 316,\n",
       " 'long': 317,\n",
       " 'look': 318,\n",
       " 'losepart': 319,\n",
       " 'loses': 320,\n",
       " 'loudly': 321,\n",
       " 'lover': 322,\n",
       " 'lovers': 323,\n",
       " 'lush': 324,\n",
       " 'lyrically': 325,\n",
       " 'lyrics': 326,\n",
       " 'maintained': 327,\n",
       " 'manzoli': 328,\n",
       " 'marriage': 329,\n",
       " 'married': 330,\n",
       " 'media': 331,\n",
       " 'melodic': 332,\n",
       " 'members': 333,\n",
       " 'merits': 334,\n",
       " 'mind': 335,\n",
       " 'minor': 336,\n",
       " 'mockingly': 337,\n",
       " 'morphing': 338,\n",
       " 'mural': 339,\n",
       " 'musical': 340,\n",
       " 'mystery': 341,\n",
       " 'names': 342,\n",
       " 'new': 343,\n",
       " 'nov': 344,\n",
       " 'ofcelebrity': 345,\n",
       " 'overshadowed': 346,\n",
       " 'paint': 347,\n",
       " 'parodied': 348,\n",
       " 'particular': 349,\n",
       " 'perception': 350,\n",
       " 'persona': 351,\n",
       " 'personal': 352,\n",
       " 'personas': 353,\n",
       " 'piano': 354,\n",
       " 'placethe': 355,\n",
       " 'play': 356,\n",
       " 'playing': 357,\n",
       " 'points': 358,\n",
       " 'poke': 359,\n",
       " 'pop': 360,\n",
       " 'potent': 361,\n",
       " 'power': 362,\n",
       " 'powerful': 363,\n",
       " 'pre': 364,\n",
       " 'previous': 365,\n",
       " 'quickly': 366,\n",
       " 'rap': 367,\n",
       " 'rapping': 368,\n",
       " 'ready': 369,\n",
       " 'real': 370,\n",
       " 'referenced': 371,\n",
       " 'referencingthe': 372,\n",
       " 'refuting': 373,\n",
       " 'relationships': 374,\n",
       " 'release': 375,\n",
       " 'released': 376,\n",
       " 'remains': 377,\n",
       " 'repeated': 378,\n",
       " 'reputation': 379,\n",
       " 'responsibility': 380,\n",
       " 'return': 381,\n",
       " 'reviled': 382,\n",
       " 'revolving': 383,\n",
       " 'richard': 384,\n",
       " 'rife': 385,\n",
       " 'right': 386,\n",
       " 'rip': 387,\n",
       " 'rising': 388,\n",
       " 'rob': 389,\n",
       " 'role': 390,\n",
       " 'romance': 391,\n",
       " 'saidfred': 392,\n",
       " 'sarcasm': 393,\n",
       " 'sass': 394,\n",
       " 'satirical': 395,\n",
       " 'satirizes': 396,\n",
       " 'savvy': 397,\n",
       " 'saw': 398,\n",
       " 'saysshe': 399,\n",
       " 'scaricature': 400,\n",
       " 'self': 401,\n",
       " 'sensibilities': 402,\n",
       " 'sensitivity': 403,\n",
       " 'sept': 404,\n",
       " 'serial': 405,\n",
       " 'seven': 406,\n",
       " 'sexy': 407,\n",
       " 'shake': 408,\n",
       " 'shaking': 409,\n",
       " 'shirking': 410,\n",
       " 'showcased': 411,\n",
       " 'sign': 412,\n",
       " 'similarly': 413,\n",
       " 'singer': 414,\n",
       " 'single': 415,\n",
       " 'singles': 416,\n",
       " 'sings': 417,\n",
       " 'somewhat': 418,\n",
       " 'song': 419,\n",
       " 'songs': 420,\n",
       " 'songwriting': 421,\n",
       " 'sonically': 422,\n",
       " 'space': 423,\n",
       " 'spersonal': 424,\n",
       " 'star': 425,\n",
       " 'stayed': 426,\n",
       " 'step': 427,\n",
       " 'story': 428,\n",
       " 'strengths': 429,\n",
       " 'stripped': 430,\n",
       " 'styles': 431,\n",
       " 'subject': 432,\n",
       " 'successfulmovie': 433,\n",
       " 'sweeter': 434,\n",
       " 'swelling': 435,\n",
       " 'swift': 436,\n",
       " 'swiftcheerfully': 437,\n",
       " 'synth': 438,\n",
       " 'takenon': 439,\n",
       " 'target': 440,\n",
       " 'taylor': 441,\n",
       " 'tells': 442,\n",
       " 'thechorus': 443,\n",
       " 'thefashion': 444,\n",
       " 'themover': 445,\n",
       " 'thesong': 446,\n",
       " 'thismedia': 447,\n",
       " 'thismysterious': 448,\n",
       " 'throbbing': 449,\n",
       " 'time': 450,\n",
       " 'tiresome': 451,\n",
       " 'title': 452,\n",
       " 'toconstructing': 453,\n",
       " 'track': 454,\n",
       " 'turned': 455,\n",
       " 'ultimate': 456,\n",
       " 'uncertain': 457,\n",
       " 'unclear': 458,\n",
       " 'understand': 459,\n",
       " 'unsatisfying': 460,\n",
       " 'use': 461,\n",
       " 'usingher': 462,\n",
       " 'vague': 463,\n",
       " 'vagueknowledge': 464,\n",
       " 'vapidity': 465,\n",
       " 'verses': 466,\n",
       " 'victim': 467,\n",
       " 'violins': 468,\n",
       " 'vulnerability': 469,\n",
       " 'wall': 470,\n",
       " 'wasfeatured': 471,\n",
       " 'week': 472,\n",
       " 'whathas': 473,\n",
       " 'widened': 474,\n",
       " 'wins': 475,\n",
       " 'work': 476,\n",
       " 'writers': 477,\n",
       " 'writes': 478,\n",
       " 'writing': 479,\n",
       " 'youknow': 480,\n",
       " 'yourlittle': 481,\n",
       " 'activity': 482,\n",
       " 'administrative': 483,\n",
       " 'alarm': 484,\n",
       " 'alarmwas': 485,\n",
       " 'appeared': 486,\n",
       " 'area': 487,\n",
       " 'arrived': 488,\n",
       " 'assistant': 489,\n",
       " 'atwoodhall': 490,\n",
       " 'bag': 491,\n",
       " 'bathroom': 492,\n",
       " 'beat': 493,\n",
       " 'brandon': 494,\n",
       " 'callaway': 495,\n",
       " 'case': 496,\n",
       " 'center': 497,\n",
       " 'checked': 498,\n",
       " 'clairmont': 499,\n",
       " 'compiled': 500,\n",
       " 'complainant': 501,\n",
       " 'concern': 502,\n",
       " 'confiscated': 503,\n",
       " 'crime': 504,\n",
       " 'department': 505,\n",
       " 'detective': 506,\n",
       " 'dobbs': 507,\n",
       " 'emory': 508,\n",
       " 'epd': 509,\n",
       " 'examined': 510,\n",
       " 'female': 511,\n",
       " 'floor': 512,\n",
       " 'food': 513,\n",
       " 'fuhr': 514,\n",
       " 'hanger': 515,\n",
       " 'information': 516,\n",
       " 'investigator': 517,\n",
       " 'iphone': 518,\n",
       " 'jeans': 519,\n",
       " 'laptop': 520,\n",
       " 'left': 521,\n",
       " 'library': 522,\n",
       " 'locate': 523,\n",
       " 'location': 524,\n",
       " 'marijuana': 525,\n",
       " 'meeting': 526,\n",
       " 'missing': 527,\n",
       " 'officer': 528,\n",
       " 'officers': 529,\n",
       " 'ofwhat': 530,\n",
       " 'police': 531,\n",
       " 'reset': 532,\n",
       " 'responded': 533,\n",
       " 'restroom': 534,\n",
       " 'returned': 535,\n",
       " 'said': 536,\n",
       " 'saidthat': 537,\n",
       " 'scene': 538,\n",
       " 'second': 539,\n",
       " 'security': 540,\n",
       " 'seen': 541,\n",
       " 'sleeping': 542,\n",
       " 'sofa': 543,\n",
       " 'someoneburned': 544,\n",
       " 'staffmember': 545,\n",
       " 'stole': 546,\n",
       " 'stolen': 547,\n",
       " 'suspicious': 548,\n",
       " 'table': 549,\n",
       " 'taken': 550,\n",
       " 'theft': 551,\n",
       " 'themon': 552,\n",
       " 'therobert': 553,\n",
       " 'tower': 554,\n",
       " 'unableto': 555,\n",
       " 'unattended': 556,\n",
       " 'university': 557,\n",
       " 'valued': 558,\n",
       " 'woodruff': 559,\n",
       " 'able': 560,\n",
       " 'ago': 561,\n",
       " 'alas': 562,\n",
       " 'americanos': 563,\n",
       " 'andfinished': 564,\n",
       " 'andgazing': 565,\n",
       " 'ask': 566,\n",
       " 'asleep': 567,\n",
       " 'aspire': 568,\n",
       " 'atlantadear': 569,\n",
       " 'atyou': 570,\n",
       " 'august': 571,\n",
       " 'awake': 572,\n",
       " 'awe': 573,\n",
       " 'backdear': 574,\n",
       " 'beenskirting': 575,\n",
       " 'better': 576,\n",
       " 'big': 577,\n",
       " 'bio': 578,\n",
       " 'bite': 579,\n",
       " 'bland': 580,\n",
       " 'body': 581,\n",
       " 'break': 582,\n",
       " 'breakfast': 583,\n",
       " 'brew': 584,\n",
       " 'bull': 585,\n",
       " 'caffeine': 586,\n",
       " 'cartoon': 587,\n",
       " 'cash': 588,\n",
       " 'changed': 589,\n",
       " 'character': 590,\n",
       " 'cheaper': 591,\n",
       " 'check': 592,\n",
       " 'chips': 593,\n",
       " 'class': 594,\n",
       " 'classes': 595,\n",
       " 'clothes': 596,\n",
       " 'clubmeetings': 597,\n",
       " 'coffee': 598,\n",
       " 'cold': 599,\n",
       " 'concoctions': 600,\n",
       " 'cook': 601,\n",
       " 'craving': 602,\n",
       " 'cup': 603,\n",
       " 'days': 604,\n",
       " 'dear': 605,\n",
       " 'dependent': 606,\n",
       " 'desk': 607,\n",
       " 'dinner': 608,\n",
       " 'discovered': 609,\n",
       " 'disgust': 610,\n",
       " 'dollars': 611,\n",
       " 'dooley': 612,\n",
       " 'dooleydollars': 613,\n",
       " 'doolino': 614,\n",
       " 'doolinodear': 615,\n",
       " 'dozens': 616,\n",
       " 'drink': 617,\n",
       " 'drinker': 618,\n",
       " 'drinks': 619,\n",
       " 'drooping': 620,\n",
       " 'drowned': 621,\n",
       " 'duc': 622,\n",
       " 'eating': 623,\n",
       " 'energy': 624,\n",
       " 'enjoy': 625,\n",
       " 'enthusiastic': 626,\n",
       " 'eyes': 627,\n",
       " 'far': 628,\n",
       " 'feed': 629,\n",
       " 'feel': 630,\n",
       " 'fell': 631,\n",
       " 'felt': 632,\n",
       " 'field': 633,\n",
       " 'filled': 634,\n",
       " 'forcing': 635,\n",
       " 'forgotten': 636,\n",
       " 'free': 637,\n",
       " 'friend': 638,\n",
       " 'friends': 639,\n",
       " 'friendship': 640,\n",
       " 'frisbee': 641,\n",
       " 'fromwonderful': 642,\n",
       " 'fruit': 643,\n",
       " 'fulfill': 644,\n",
       " 'goer': 645,\n",
       " 'good': 646,\n",
       " 'got': 647,\n",
       " 'grades': 648,\n",
       " 'ground': 649,\n",
       " 'guy': 650,\n",
       " 'hallmates': 651,\n",
       " 'hang': 652,\n",
       " 'happened': 653,\n",
       " 'hasn': 654,\n",
       " 'haven': 655,\n",
       " 'health': 656,\n",
       " 'heard': 657,\n",
       " 'hearty': 658,\n",
       " 'hippo': 659,\n",
       " 'hippodear': 660,\n",
       " 'hour': 661,\n",
       " 'hungry': 662,\n",
       " 'imagining': 663,\n",
       " 'important': 664,\n",
       " 'instead': 665,\n",
       " 'interrupted': 666,\n",
       " 'isnothing': 667,\n",
       " 'lategym': 668,\n",
       " 'laundry': 669,\n",
       " 'letter': 670,\n",
       " 'ling': 671,\n",
       " 'live': 672,\n",
       " 'love': 673,\n",
       " 'mcdonough': 674,\n",
       " 'meal': 675,\n",
       " 'meat': 676,\n",
       " 'methods': 677,\n",
       " 'midterm': 678,\n",
       " 'moderation': 679,\n",
       " 'moment': 680,\n",
       " 'morning': 681,\n",
       " 'munching': 682,\n",
       " 'need': 683,\n",
       " 'night': 684,\n",
       " 'nights': 685,\n",
       " 'nonetheless': 686,\n",
       " 'nose': 687,\n",
       " 'notice': 688,\n",
       " 'obviously': 689,\n",
       " 'onher': 690,\n",
       " 'opt': 691,\n",
       " 'opting': 692,\n",
       " 'othersjust': 693,\n",
       " 'pagedocument': 694,\n",
       " 'paper': 695,\n",
       " 'pasta': 696,\n",
       " 'peek': 697,\n",
       " 'people': 698,\n",
       " 'periodically': 699,\n",
       " 'phone': 700,\n",
       " 'potato': 701,\n",
       " 'practically': 702,\n",
       " 'practice': 703,\n",
       " 'prefer': 704,\n",
       " 'professor': 705,\n",
       " 'quick': 706,\n",
       " 'quiz': 707,\n",
       " 'recoiled': 708,\n",
       " 'red': 709,\n",
       " 'remember': 710,\n",
       " 'run': 711,\n",
       " 'salad': 712,\n",
       " 'sat': 713,\n",
       " 'saturate': 714,\n",
       " 'sauce': 715,\n",
       " 'scrolling': 716,\n",
       " 'secret': 717,\n",
       " 'selection': 718,\n",
       " 'shirt': 719,\n",
       " 'shower': 720,\n",
       " 'signed': 721,\n",
       " 'sincerely': 722,\n",
       " 'slap': 723,\n",
       " 'soulless': 724,\n",
       " 'stand': 725,\n",
       " 'starting': 726,\n",
       " 'stay': 727,\n",
       " 'steppedinto': 728,\n",
       " 'stroll': 729,\n",
       " 'struggle': 730,\n",
       " 'study': 731,\n",
       " 'suitemate': 732,\n",
       " 'sure': 733,\n",
       " 'taste': 734,\n",
       " 'tea': 735,\n",
       " 'tgorge': 736,\n",
       " 'thefood': 737,\n",
       " 'theoretically': 738,\n",
       " 'thepasta': 739,\n",
       " 'things': 740,\n",
       " 'think': 741,\n",
       " 'times': 742,\n",
       " 'told': 743,\n",
       " 'trick': 744,\n",
       " 'true': 745,\n",
       " 'truly': 746,\n",
       " 'turn': 747,\n",
       " 'turnedtoward': 748,\n",
       " 'unforgiving': 749,\n",
       " 'unsure': 750,\n",
       " 'venti': 751,\n",
       " 'venture': 752,\n",
       " 'want': 753,\n",
       " 'warm': 754,\n",
       " 'way': 755,\n",
       " 'wearing': 756,\n",
       " 'wednesday': 757,\n",
       " 'weekly': 758,\n",
       " 'weeks': 759,\n",
       " 'whowears': 760,\n",
       " 'willonly': 761,\n",
       " 'won': 762,\n",
       " 'wrinkled': 763,\n",
       " 'wrong': 764,\n",
       " 'yes': 765,\n",
       " 'youcan': 766,\n",
       " 'academic': 767,\n",
       " 'academy': 768,\n",
       " 'added': 769,\n",
       " 'advisees': 770,\n",
       " 'african': 771,\n",
       " 'agreed': 772,\n",
       " 'american': 773,\n",
       " 'andhistory': 774,\n",
       " 'arts': 775,\n",
       " 'associate': 776,\n",
       " 'attract': 777,\n",
       " 'beautiful': 778,\n",
       " 'becrippling': 779,\n",
       " 'beganrethinking': 780,\n",
       " 'biggesthope': 781,\n",
       " 'blackregiments': 782,\n",
       " 'book': 783,\n",
       " 'brett': 784,\n",
       " 'brown': 785,\n",
       " 'built': 786,\n",
       " 'butnow': 787,\n",
       " 'calling': 788,\n",
       " 'campus': 789,\n",
       " 'chair': 790,\n",
       " 'challenges': 791,\n",
       " 'cohen': 792,\n",
       " 'colleague': 793,\n",
       " 'colleagues': 794,\n",
       " 'collection': 795,\n",
       " 'college': 796,\n",
       " 'comes': 797,\n",
       " 'continue': 798,\n",
       " 'continues': 799,\n",
       " 'creative': 800,\n",
       " 'creativewriting': 801,\n",
       " 'cultivate': 802,\n",
       " 'currently': 803,\n",
       " 'dean': 804,\n",
       " 'decline': 805,\n",
       " 'degree': 806,\n",
       " 'develop': 807,\n",
       " 'director': 808,\n",
       " 'elliott': 809,\n",
       " 'endowed': 810,\n",
       " 'english': 811,\n",
       " 'evanston': 812,\n",
       " 'evengreater': 813,\n",
       " 'exciting': 814,\n",
       " 'extraordinary': 815,\n",
       " 'extremely': 816,\n",
       " 'faculty': 817,\n",
       " 'fine': 818,\n",
       " 'form': 819,\n",
       " 'forwardto': 820,\n",
       " 'future': 821,\n",
       " 'gadsden': 822,\n",
       " 'glad': 823,\n",
       " 'going': 824,\n",
       " 'graduate': 825,\n",
       " 'guards': 826,\n",
       " 'haselevated': 827,\n",
       " 'hebelieves': 828,\n",
       " 'hermother': 829,\n",
       " 'hopes': 830,\n",
       " 'husband': 831,\n",
       " 'ill': 832,\n",
       " 'illinois': 833,\n",
       " 'impulses': 834,\n",
       " 'includes': 835,\n",
       " 'inducted': 836,\n",
       " 'inspiration': 837,\n",
       " 'inspired': 838,\n",
       " 'interested': 839,\n",
       " 'interim': 840,\n",
       " 'intothe': 841,\n",
       " 'invitations': 842,\n",
       " 'january': 843,\n",
       " 'jericho': 844,\n",
       " 'join': 845,\n",
       " 'laureate': 846,\n",
       " 'leave': 847,\n",
       " 'literature': 848,\n",
       " 'looking': 849,\n",
       " 'losing': 850,\n",
       " 'louisiana': 851,\n",
       " 'main': 852,\n",
       " 'maintain': 853,\n",
       " 'master': 854,\n",
       " 'michael': 855,\n",
       " 'moved': 856,\n",
       " 'natasha': 857,\n",
       " 'native': 858,\n",
       " 'news': 859,\n",
       " 'northwestern': 860,\n",
       " 'ofdevelopment': 861,\n",
       " 'offers': 862,\n",
       " 'official': 863,\n",
       " 'opportunity': 864,\n",
       " 'orit': 865,\n",
       " 'pen': 866,\n",
       " 'phase': 867,\n",
       " 'poet': 868,\n",
       " 'poetry': 869,\n",
       " 'presence': 870,\n",
       " 'prize': 871,\n",
       " 'professors': 872,\n",
       " 'program': 873,\n",
       " 'programs': 874,\n",
       " 'prose': 875,\n",
       " 'proud': 876,\n",
       " 'provideher': 877,\n",
       " 'pulitzer': 878,\n",
       " 'raised': 879,\n",
       " 'received': 880,\n",
       " 'recently': 881,\n",
       " 'recognized': 882,\n",
       " 'regiment': 883,\n",
       " 'relationship': 884,\n",
       " 'renewed': 885,\n",
       " 'research': 886,\n",
       " 'robert': 887,\n",
       " 'roots': 888,\n",
       " 'scenery': 889,\n",
       " 'sciences': 890,\n",
       " 'seeking': 891,\n",
       " 'sets': 892,\n",
       " 'similar': 893,\n",
       " 'slegacy': 894,\n",
       " 'smaller': 895,\n",
       " 'sophomore': 896,\n",
       " 'southern': 897,\n",
       " 'spell': 898,\n",
       " 'standard': 899,\n",
       " 'staturefurther': 900,\n",
       " 'strong': 901,\n",
       " 'students': 902,\n",
       " 'studies': 903,\n",
       " 'success': 904,\n",
       " 'taught': 905,\n",
       " 'teach': 906,\n",
       " 'teacher': 907,\n",
       " 'teaching': 908,\n",
       " 'term': 909,\n",
       " 'thebest': 910,\n",
       " 'theu': 911,\n",
       " 'tothe': 912,\n",
       " 'tragedies': 913,\n",
       " 'tremendous': 914,\n",
       " 'tretheway': 915,\n",
       " 'trethewey': 916,\n",
       " 'tretheweywill': 917,\n",
       " 'tribute': 918,\n",
       " 'trustees': 919,\n",
       " 'undergraduate': 920,\n",
       " 'union': 921,\n",
       " 'unlike': 922,\n",
       " 'veryhurt': 923,\n",
       " 'willallow': 924,\n",
       " 'winning': 925,\n",
       " 'working': 926,\n",
       " 'workingon': 927,\n",
       " 'works': 928,\n",
       " 'workshops': 929,\n",
       " 'wouldn': 930,\n",
       " 'write': 931,\n",
       " 'wrote': 932,\n",
       " 'year': 933,\n",
       " 'according': 934,\n",
       " 'advises': 935,\n",
       " 'advisory': 936,\n",
       " 'anduniversity': 937,\n",
       " 'atabout': 938,\n",
       " 'bathe': 939,\n",
       " 'boil': 940,\n",
       " 'bottled': 941,\n",
       " 'campusdining': 942,\n",
       " 'candler': 943,\n",
       " 'candlerwater': 944,\n",
       " 'caused': 945,\n",
       " 'city': 946,\n",
       " 'communications': 947,\n",
       " 'community': 948,\n",
       " 'dekalbcounty': 949,\n",
       " 'division': 950,\n",
       " 'drinking': 951,\n",
       " 'dropbelow': 952,\n",
       " 'dunwoody': 953,\n",
       " 'email': 954,\n",
       " 'emailed': 955,\n",
       " 'emergency': 956,\n",
       " 'ensured': 957,\n",
       " 'entirecounty': 958,\n",
       " 'environmental': 959,\n",
       " 'housing': 960,\n",
       " 'informed': 961,\n",
       " 'issued': 962,\n",
       " 'issues': 963,\n",
       " 'lifted': 964,\n",
       " 'locations': 965,\n",
       " 'management': 966,\n",
       " 'menus': 967,\n",
       " 'minutes': 968,\n",
       " 'modify': 969,\n",
       " 'occurred': 970,\n",
       " 'orcooking': 971,\n",
       " 'outages': 972,\n",
       " 'plant': 973,\n",
       " 'post': 974,\n",
       " 'pressure': 975,\n",
       " 'prior': 976,\n",
       " 'protection': 977,\n",
       " 'provide': 978,\n",
       " 'remain': 979,\n",
       " 'requirements': 980,\n",
       " 'residents': 981,\n",
       " 'safe': 982,\n",
       " 'safety': 983,\n",
       " 'saturday': 984,\n",
       " 'scott': 985,\n",
       " 'sunday': 986,\n",
       " 'swallowed': 987,\n",
       " 'thunderstorms': 988,\n",
       " 'treatment': 989,\n",
       " 'untillifted': 990,\n",
       " 'update': 991,\n",
       " 'usage': 992,\n",
       " 'warn': 993,\n",
       " 'warning': 994,\n",
       " 'water': 995,\n",
       " 'watershed': 996,\n",
       " 'wheel': 997,\n",
       " 'wide': 998,\n",
       " 'acar': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word_wheel.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out common words / uncommon words ##\n",
    "\n",
    "There aren't many things you need to do in order to tune your topic model, but one important thing do consider is whether you should filter some of the words. \n",
    "\n",
    "gensim provides two basic functions for this, one which filters out the top n most frequent words, and another which filters out the words that appear in fewer (or greater) than a certain number of documents. \n",
    "\n",
    "We'll try out both below, even though you may end up not needing / wanting to use both (or either) of these functions for your particular purpose. This is one of the places where experience (and trial and error) will tell you what's appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : discarding 10 tokens: [('emory', 2971), ('said', 2413), ('time', 2273), ('like', 2080), ('year', 1880), ('university', 1869), ('college', 1825), ('new', 1810), ('people', 1750), ('wheel', 1679)]...\n",
      "INFO : resulting dictionary: Dictionary(118775 unique tokens: ['acute', 'adorn', 'afterthe', 'allow', 'amend']...)\n",
      "INFO : discarding 75617 tokens: [('amonument', 1), ('andserved', 1), ('causemonument', 1), ('foroppression', 1), ('gazebo', 1), ('slaveowner', 1), ('thedeclaration', 1), ('themonument', 1), ('theremoval', 1), ('touristy', 1)]...\n",
      "INFO : keeping 43158 tokens which were in no less than 2 and no more than 4009 (=100.0%) documents\n",
      "INFO : resulting dictionary: Dictionary(43158 unique tokens: ['acute', 'adorn', 'afterthe', 'allow', 'amend']...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7fc0890f6550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this line filters out the 10 most frequent words\n",
    "id2word_wheel.filter_n_most_frequent(10)\n",
    "\n",
    "# this line filters out words that appear only 1 doc, keeping the rest\n",
    "# note how no_below and no_above take different values\n",
    "id2word_wheel.filter_extremes(no_below=2, no_above=1.0)\n",
    "\n",
    "id2word_wheel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by removing the words that only appeared in a single document, we went from 118,786 unique words (or tokens) to 43,158. That's not a huge number for a topic model, but we'll see how it goes... \n",
    "\n",
    "### Final set-up\n",
    "\n",
    "We need to do only a few more things in order to run our topic model. 1) Define a special class for working with corpora (again, per the gensim documentation); and 2) use the Corpus class to create a stream of bag-of-words vectors. These two chunks of code are as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Corpus class \n",
    "# this is the same for every topic model you create with gensim. \n",
    "# no need to modify it here\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, dump_file, dictionary, clip_docs=None):\n",
    "        self.dump_file = dump_file\n",
    "        self.dictionary = dictionary\n",
    "        self.clip_docs = clip_docs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        for title, tokens in itertools.islice(iter_docs(self.dump_file), self.clip_docs):\n",
    "            self.titles.append(title)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.clip_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Corpus at 0x7fc0898e5240>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating our stream of bag-of-words vectors\n",
    "## MODIFY PATH IF YOU'RE GETTING FILE NOT FOUND ERRORS\n",
    "wheel_corpus = Corpus('../corpora/emory-wheel/articles/', id2word_wheel) \n",
    "\n",
    "wheel_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running our topic model! \n",
    "\n",
    "At long last, we're ready to run our topic model. Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : using symmetric alpha at 0.06666666666666667\n",
      "INFO : using symmetric eta at 0.06666666666666667\n",
      "INFO : using serial LDA version on this node\n",
      "WARNING : input corpus stream has no len(); counting documents\n",
      "INFO : running online (multi-pass) LDA training, 15 topics, 5 passes over the supplied corpus of 4009 documents, updating model once every 2000 documents, evaluating perplexity every 4009 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "INFO : PROGRESS: pass 0, at document #2000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #2 (0.067): 0.007*\"students\" + 0.004*\"sga\" + 0.003*\"film\" + 0.003*\"student\" + 0.003*\"school\" + 0.003*\"campus\" + 0.002*\"think\" + 0.002*\"atlanta\" + 0.002*\"years\" + 0.002*\"team\"\n",
      "INFO : topic #5 (0.067): 0.006*\"team\" + 0.005*\"student\" + 0.005*\"students\" + 0.003*\"eagles\" + 0.003*\"life\" + 0.003*\"campus\" + 0.003*\"game\" + 0.003*\"according\" + 0.002*\"second\" + 0.002*\"film\"\n",
      "INFO : topic #10 (0.067): 0.007*\"students\" + 0.004*\"life\" + 0.003*\"according\" + 0.003*\"film\" + 0.003*\"campus\" + 0.003*\"world\" + 0.002*\"president\" + 0.002*\"years\" + 0.002*\"school\" + 0.002*\"state\"\n",
      "INFO : topic #1 (0.067): 0.006*\"students\" + 0.006*\"team\" + 0.004*\"student\" + 0.003*\"according\" + 0.003*\"game\" + 0.003*\"film\" + 0.003*\"president\" + 0.003*\"season\" + 0.002*\"epd\" + 0.002*\"campus\"\n",
      "INFO : topic #12 (0.067): 0.005*\"students\" + 0.005*\"student\" + 0.003*\"atlanta\" + 0.003*\"school\" + 0.003*\"game\" + 0.003*\"according\" + 0.002*\"team\" + 0.002*\"work\" + 0.002*\"film\" + 0.002*\"eagles\"\n",
      "INFO : topic diff=5.252235, rho=1.000000\n",
      "INFO : PROGRESS: pass 0, at document #4000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #9 (0.067): 0.006*\"film\" + 0.004*\"students\" + 0.004*\"world\" + 0.003*\"president\" + 0.003*\"campus\" + 0.002*\"love\" + 0.002*\"life\" + 0.002*\"story\" + 0.002*\"way\" + 0.002*\"team\"\n",
      "INFO : topic #14 (0.067): 0.010*\"team\" + 0.008*\"eagles\" + 0.008*\"game\" + 0.005*\"students\" + 0.005*\"senior\" + 0.004*\"season\" + 0.004*\"second\" + 0.004*\"goal\" + 0.004*\"junior\" + 0.003*\"sophomore\"\n",
      "INFO : topic #11 (0.067): 0.014*\"team\" + 0.012*\"eagles\" + 0.007*\"season\" + 0.005*\"senior\" + 0.005*\"game\" + 0.005*\"sophomore\" + 0.004*\"points\" + 0.004*\"match\" + 0.004*\"second\" + 0.004*\"win\"\n",
      "INFO : topic #4 (0.067): 0.004*\"student\" + 0.003*\"trump\" + 0.003*\"students\" + 0.003*\"atlanta\" + 0.003*\"according\" + 0.003*\"school\" + 0.003*\"health\" + 0.002*\"life\" + 0.002*\"percent\" + 0.002*\"coffee\"\n",
      "INFO : topic #12 (0.067): 0.006*\"atlanta\" + 0.004*\"students\" + 0.003*\"game\" + 0.003*\"student\" + 0.003*\"school\" + 0.002*\"according\" + 0.002*\"week\" + 0.002*\"post\" + 0.002*\"music\" + 0.002*\"georgia\"\n",
      "INFO : topic diff=1.981193, rho=0.707107\n",
      "INFO : -9.692 per-word bound, 827.3 perplexity estimate based on a held-out corpus of 9 documents with 3490 words\n",
      "INFO : PROGRESS: pass 0, at document #4009/4009\n",
      "INFO : merging changes from 9 documents into a model of 4009 documents\n",
      "INFO : topic #7 (0.067): 0.023*\"provost\" + 0.017*\"students\" + 0.015*\"mcbride\" + 0.014*\"vice\" + 0.012*\"graduate\" + 0.011*\"office\" + 0.010*\"according\" + 0.009*\"faculty\" + 0.007*\"vigil\" + 0.007*\"wrote\"\n",
      "INFO : topic #13 (0.067): 0.006*\"evacuation\" + 0.004*\"students\" + 0.004*\"according\" + 0.004*\"sherlock\" + 0.004*\"life\" + 0.003*\"campus\" + 0.003*\"food\" + 0.003*\"house\" + 0.002*\"atlanta\" + 0.002*\"music\"\n",
      "INFO : topic #14 (0.067): 0.010*\"team\" + 0.007*\"game\" + 0.007*\"eagles\" + 0.005*\"students\" + 0.005*\"senior\" + 0.005*\"season\" + 0.005*\"goal\" + 0.004*\"second\" + 0.004*\"junior\" + 0.003*\"groove\"\n",
      "INFO : topic #10 (0.067): 0.011*\"trump\" + 0.010*\"law\" + 0.009*\"students\" + 0.009*\"political\" + 0.007*\"wax\" + 0.006*\"school\" + 0.006*\"protesters\" + 0.005*\"campus\" + 0.005*\"right\" + 0.005*\"believe\"\n",
      "INFO : topic #6 (0.067): 0.017*\"april\" + 0.015*\"officer\" + 0.011*\"individual\" + 0.010*\"student\" + 0.009*\"reported\" + 0.009*\"epd\" + 0.008*\"subject\" + 0.007*\"caller\" + 0.006*\"wallet\" + 0.006*\"campus\"\n",
      "INFO : topic diff=1.835440, rho=0.577350\n",
      "INFO : PROGRESS: pass 1, at document #2000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #11 (0.067): 0.040*\"team\" + 0.022*\"season\" + 0.014*\"su\" + 0.014*\"spring\" + 0.013*\"matches\" + 0.013*\"tournament\" + 0.011*\"winning\" + 0.011*\"wax\" + 0.010*\"fall\" + 0.009*\"play\"\n",
      "INFO : topic #14 (0.067): 0.010*\"team\" + 0.006*\"eagles\" + 0.005*\"senior\" + 0.005*\"game\" + 0.004*\"second\" + 0.004*\"students\" + 0.004*\"season\" + 0.004*\"junior\" + 0.003*\"goal\" + 0.003*\"men\"\n",
      "INFO : topic #4 (0.067): 0.021*\"child\" + 0.018*\"euh\" + 0.012*\"surgery\" + 0.011*\"boazman\" + 0.010*\"transplant\" + 0.010*\"rice\" + 0.009*\"burgess\" + 0.009*\"family\" + 0.008*\"baby\" + 0.008*\"dickerson\"\n",
      "INFO : topic #10 (0.067): 0.010*\"trump\" + 0.010*\"students\" + 0.010*\"law\" + 0.008*\"political\" + 0.006*\"school\" + 0.005*\"wax\" + 0.005*\"campus\" + 0.005*\"rights\" + 0.005*\"right\" + 0.005*\"protesters\"\n",
      "INFO : topic #9 (0.067): 0.005*\"audience\" + 0.005*\"performance\" + 0.005*\"music\" + 0.005*\"socks\" + 0.005*\"father\" + 0.005*\"songs\" + 0.004*\"young\" + 0.004*\"pilots\" + 0.004*\"unionization\" + 0.004*\"dun\"\n",
      "INFO : topic diff=1.253231, rho=0.499719\n",
      "INFO : PROGRESS: pass 1, at document #4000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #8 (0.067): 0.006*\"students\" + 0.006*\"student\" + 0.005*\"work\" + 0.004*\"course\" + 0.003*\"art\" + 0.003*\"major\" + 0.003*\"honor\" + 0.003*\"professor\" + 0.003*\"program\" + 0.003*\"carter\"\n",
      "INFO : topic #1 (0.067): 0.030*\"subject\" + 0.025*\"officer\" + 0.022*\"complainant\" + 0.017*\"epd\" + 0.015*\"reported\" + 0.012*\"assigned\" + 0.011*\"case\" + 0.010*\"april\" + 0.009*\"investigator\" + 0.008*\"wallet\"\n",
      "INFO : topic #5 (0.067): 0.017*\"song\" + 0.014*\"songs\" + 0.013*\"album\" + 0.013*\"music\" + 0.008*\"band\" + 0.008*\"performance\" + 0.006*\"audience\" + 0.006*\"concert\" + 0.006*\"stage\" + 0.006*\"crowd\"\n",
      "INFO : topic #13 (0.067): 0.006*\"according\" + 0.005*\"food\" + 0.005*\"life\" + 0.005*\"campus\" + 0.004*\"students\" + 0.004*\"atlanta\" + 0.004*\"house\" + 0.003*\"center\" + 0.003*\"community\" + 0.003*\"director\"\n",
      "INFO : topic #12 (0.067): 0.011*\"music\" + 0.010*\"atlanta\" + 0.009*\"songs\" + 0.007*\"stage\" + 0.007*\"audience\" + 0.007*\"crowd\" + 0.006*\"band\" + 0.006*\"song\" + 0.006*\"killers\" + 0.005*\"performance\"\n",
      "INFO : topic diff=0.995833, rho=0.499719\n",
      "INFO : -7.506 per-word bound, 181.7 perplexity estimate based on a held-out corpus of 9 documents with 3490 words\n",
      "INFO : PROGRESS: pass 1, at document #4009/4009\n",
      "INFO : merging changes from 9 documents into a model of 4009 documents\n",
      "INFO : topic #2 (0.067): 0.009*\"doo\" + 0.008*\"students\" + 0.005*\"student\" + 0.005*\"school\" + 0.004*\"campus\" + 0.004*\"trump\" + 0.004*\"president\" + 0.003*\"black\" + 0.003*\"sga\" + 0.003*\"georgia\"\n",
      "INFO : topic #11 (0.067): 0.048*\"team\" + 0.027*\"season\" + 0.021*\"su\" + 0.021*\"spring\" + 0.016*\"tournament\" + 0.015*\"matches\" + 0.015*\"winning\" + 0.014*\"fall\" + 0.013*\"wrote\" + 0.012*\"play\"\n",
      "INFO : topic #8 (0.067): 0.006*\"students\" + 0.005*\"student\" + 0.005*\"work\" + 0.004*\"course\" + 0.003*\"art\" + 0.003*\"major\" + 0.003*\"honor\" + 0.003*\"professor\" + 0.003*\"program\" + 0.003*\"carter\"\n",
      "INFO : topic #5 (0.067): 0.022*\"songs\" + 0.019*\"music\" + 0.017*\"song\" + 0.014*\"performance\" + 0.013*\"audience\" + 0.010*\"stage\" + 0.009*\"concert\" + 0.008*\"logic\" + 0.008*\"band\" + 0.008*\"crowd\"\n",
      "INFO : topic #3 (0.067): 0.022*\"game\" + 0.012*\"eagles\" + 0.009*\"team\" + 0.008*\"points\" + 0.007*\"season\" + 0.006*\"win\" + 0.006*\"second\" + 0.006*\"games\" + 0.005*\"lead\" + 0.005*\"run\"\n",
      "INFO : topic diff=1.288532, rho=0.499719\n",
      "INFO : PROGRESS: pass 2, at document #2000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #11 (0.067): 0.048*\"team\" + 0.027*\"season\" + 0.018*\"spring\" + 0.018*\"su\" + 0.016*\"tournament\" + 0.015*\"matches\" + 0.014*\"winning\" + 0.013*\"fall\" + 0.011*\"wrote\" + 0.011*\"singles\"\n",
      "INFO : topic #2 (0.067): 0.009*\"students\" + 0.005*\"student\" + 0.004*\"president\" + 0.004*\"school\" + 0.004*\"campus\" + 0.004*\"trump\" + 0.003*\"black\" + 0.003*\"government\" + 0.003*\"sga\" + 0.003*\"state\"\n",
      "INFO : topic #7 (0.067): 0.027*\"students\" + 0.023*\"provost\" + 0.017*\"mcbride\" + 0.017*\"vice\" + 0.016*\"faculty\" + 0.014*\"graduate\" + 0.014*\"office\" + 0.012*\"education\" + 0.011*\"according\" + 0.008*\"hopes\"\n",
      "INFO : topic #8 (0.067): 0.007*\"students\" + 0.006*\"student\" + 0.005*\"work\" + 0.005*\"course\" + 0.004*\"professor\" + 0.004*\"carter\" + 0.004*\"honor\" + 0.004*\"art\" + 0.003*\"science\" + 0.003*\"major\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic #10 (0.067): 0.014*\"law\" + 0.014*\"trump\" + 0.012*\"wax\" + 0.011*\"political\" + 0.009*\"students\" + 0.008*\"school\" + 0.006*\"action\" + 0.006*\"protesters\" + 0.006*\"right\" + 0.006*\"believe\"\n",
      "INFO : topic diff=0.749711, rho=0.447012\n",
      "INFO : PROGRESS: pass 2, at document #4000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #3 (0.067): 0.027*\"game\" + 0.016*\"eagles\" + 0.012*\"team\" + 0.009*\"points\" + 0.008*\"season\" + 0.007*\"win\" + 0.007*\"games\" + 0.007*\"second\" + 0.006*\"lead\" + 0.006*\"players\"\n",
      "INFO : topic #8 (0.067): 0.008*\"students\" + 0.006*\"student\" + 0.006*\"work\" + 0.006*\"course\" + 0.005*\"professor\" + 0.005*\"carter\" + 0.004*\"major\" + 0.004*\"arts\" + 0.004*\"art\" + 0.004*\"science\"\n",
      "INFO : topic #13 (0.067): 0.007*\"according\" + 0.006*\"campus\" + 0.006*\"food\" + 0.005*\"life\" + 0.005*\"atlanta\" + 0.005*\"students\" + 0.004*\"center\" + 0.003*\"health\" + 0.003*\"community\" + 0.003*\"house\"\n",
      "INFO : topic #11 (0.067): 0.049*\"team\" + 0.027*\"season\" + 0.018*\"tournament\" + 0.015*\"spring\" + 0.014*\"su\" + 0.013*\"matches\" + 0.013*\"winning\" + 0.012*\"singles\" + 0.011*\"set\" + 0.011*\"play\"\n",
      "INFO : topic #12 (0.067): 0.011*\"atlanta\" + 0.010*\"music\" + 0.010*\"songs\" + 0.009*\"audience\" + 0.008*\"stage\" + 0.007*\"killers\" + 0.007*\"word\" + 0.007*\"crowd\" + 0.007*\"band\" + 0.006*\"rock\"\n",
      "INFO : topic diff=0.563736, rho=0.447012\n",
      "INFO : -6.894 per-word bound, 118.9 perplexity estimate based on a held-out corpus of 9 documents with 3490 words\n",
      "INFO : PROGRESS: pass 2, at document #4009/4009\n",
      "INFO : merging changes from 9 documents into a model of 4009 documents\n",
      "INFO : topic #5 (0.067): 0.027*\"songs\" + 0.023*\"music\" + 0.018*\"song\" + 0.015*\"audience\" + 0.015*\"performance\" + 0.013*\"stage\" + 0.011*\"crowd\" + 0.010*\"concert\" + 0.010*\"band\" + 0.009*\"album\"\n",
      "INFO : topic #8 (0.067): 0.008*\"students\" + 0.006*\"student\" + 0.005*\"work\" + 0.005*\"course\" + 0.005*\"professor\" + 0.005*\"carter\" + 0.004*\"major\" + 0.004*\"arts\" + 0.004*\"art\" + 0.004*\"science\"\n",
      "INFO : topic #7 (0.067): 0.033*\"provost\" + 0.026*\"mcbride\" + 0.025*\"students\" + 0.020*\"vice\" + 0.019*\"faculty\" + 0.017*\"graduate\" + 0.017*\"office\" + 0.016*\"education\" + 0.011*\"hopes\" + 0.009*\"want\"\n",
      "INFO : topic #2 (0.067): 0.009*\"students\" + 0.007*\"doo\" + 0.005*\"student\" + 0.005*\"president\" + 0.004*\"campus\" + 0.004*\"trump\" + 0.004*\"school\" + 0.003*\"themotivations\" + 0.003*\"state\" + 0.003*\"georgia\"\n",
      "INFO : topic #14 (0.067): 0.011*\"team\" + 0.006*\"senior\" + 0.005*\"eagles\" + 0.005*\"women\" + 0.005*\"place\" + 0.005*\"second\" + 0.004*\"men\" + 0.004*\"freshman\" + 0.004*\"goal\" + 0.004*\"junior\"\n",
      "INFO : topic diff=0.926023, rho=0.447012\n",
      "INFO : PROGRESS: pass 3, at document #2000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #0 (0.067): 0.017*\"film\" + 0.005*\"story\" + 0.004*\"world\" + 0.004*\"characters\" + 0.004*\"audience\" + 0.004*\"movie\" + 0.004*\"love\" + 0.003*\"character\" + 0.003*\"best\" + 0.003*\"life\"\n",
      "INFO : topic #10 (0.067): 0.016*\"law\" + 0.015*\"trump\" + 0.014*\"wax\" + 0.012*\"political\" + 0.009*\"students\" + 0.009*\"school\" + 0.007*\"action\" + 0.007*\"protesters\" + 0.006*\"right\" + 0.006*\"believe\"\n",
      "INFO : topic #14 (0.067): 0.012*\"team\" + 0.006*\"senior\" + 0.005*\"women\" + 0.005*\"place\" + 0.005*\"eagles\" + 0.005*\"second\" + 0.005*\"men\" + 0.005*\"freshman\" + 0.004*\"junior\" + 0.004*\"sophomore\"\n",
      "INFO : topic #13 (0.067): 0.008*\"according\" + 0.007*\"campus\" + 0.006*\"atlanta\" + 0.006*\"life\" + 0.005*\"students\" + 0.005*\"food\" + 0.004*\"center\" + 0.004*\"health\" + 0.003*\"community\" + 0.003*\"house\"\n",
      "INFO : topic #1 (0.067): 0.034*\"officer\" + 0.032*\"subject\" + 0.022*\"april\" + 0.018*\"reported\" + 0.018*\"complainant\" + 0.016*\"epd\" + 0.013*\"wallet\" + 0.011*\"assigned\" + 0.011*\"investigator\" + 0.011*\"case\"\n",
      "INFO : topic diff=0.523762, rho=0.408095\n",
      "INFO : PROGRESS: pass 3, at document #4000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #9 (0.067): 0.010*\"audience\" + 0.009*\"father\" + 0.008*\"performance\" + 0.007*\"socks\" + 0.006*\"music\" + 0.006*\"belgian\" + 0.006*\"logic\" + 0.006*\"young\" + 0.005*\"dressed\" + 0.005*\"rae\"\n",
      "INFO : topic #13 (0.067): 0.009*\"according\" + 0.007*\"campus\" + 0.006*\"atlanta\" + 0.006*\"food\" + 0.006*\"students\" + 0.005*\"life\" + 0.004*\"center\" + 0.004*\"health\" + 0.003*\"community\" + 0.003*\"director\"\n",
      "INFO : topic #8 (0.067): 0.010*\"students\" + 0.007*\"professor\" + 0.007*\"student\" + 0.006*\"course\" + 0.006*\"work\" + 0.006*\"arts\" + 0.006*\"carter\" + 0.005*\"program\" + 0.005*\"major\" + 0.005*\"science\"\n",
      "INFO : topic #1 (0.067): 0.033*\"officer\" + 0.033*\"subject\" + 0.022*\"april\" + 0.020*\"complainant\" + 0.018*\"reported\" + 0.016*\"epd\" + 0.012*\"wallet\" + 0.012*\"assigned\" + 0.011*\"investigator\" + 0.011*\"case\"\n",
      "INFO : topic #12 (0.067): 0.011*\"atlanta\" + 0.010*\"word\" + 0.009*\"kesha\" + 0.006*\"japanese\" + 0.006*\"killers\" + 0.006*\"rock\" + 0.006*\"school\" + 0.006*\"star\" + 0.005*\"zoom\" + 0.005*\"band\"\n",
      "INFO : topic diff=0.397137, rho=0.408095\n",
      "INFO : -6.672 per-word bound, 102.0 perplexity estimate based on a held-out corpus of 9 documents with 3490 words\n",
      "INFO : PROGRESS: pass 3, at document #4009/4009\n",
      "INFO : merging changes from 9 documents into a model of 4009 documents\n",
      "INFO : topic #1 (0.067): 0.034*\"officer\" + 0.032*\"subject\" + 0.023*\"april\" + 0.018*\"reported\" + 0.017*\"complainant\" + 0.016*\"epd\" + 0.013*\"wallet\" + 0.011*\"assigned\" + 0.011*\"investigator\" + 0.011*\"case\"\n",
      "INFO : topic #9 (0.067): 0.011*\"audience\" + 0.009*\"father\" + 0.008*\"performance\" + 0.008*\"killers\" + 0.007*\"socks\" + 0.007*\"belgian\" + 0.007*\"music\" + 0.007*\"dressed\" + 0.006*\"logic\" + 0.006*\"young\"\n",
      "INFO : topic #11 (0.067): 0.050*\"team\" + 0.029*\"season\" + 0.022*\"spring\" + 0.022*\"su\" + 0.017*\"tournament\" + 0.016*\"matches\" + 0.016*\"winning\" + 0.015*\"fall\" + 0.014*\"wrote\" + 0.012*\"play\"\n",
      "INFO : topic #8 (0.067): 0.010*\"students\" + 0.007*\"professor\" + 0.006*\"student\" + 0.006*\"course\" + 0.006*\"work\" + 0.006*\"arts\" + 0.005*\"carter\" + 0.005*\"program\" + 0.005*\"major\" + 0.005*\"science\"\n",
      "INFO : topic #5 (0.067): 0.029*\"songs\" + 0.024*\"music\" + 0.016*\"audience\" + 0.016*\"song\" + 0.016*\"performance\" + 0.014*\"stage\" + 0.013*\"crowd\" + 0.011*\"band\" + 0.010*\"concert\" + 0.009*\"album\"\n",
      "INFO : topic diff=0.738108, rho=0.408095\n",
      "INFO : PROGRESS: pass 4, at document #2000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #14 (0.067): 0.013*\"team\" + 0.006*\"senior\" + 0.006*\"women\" + 0.006*\"place\" + 0.005*\"men\" + 0.005*\"second\" + 0.005*\"freshman\" + 0.005*\"eagles\" + 0.004*\"junior\" + 0.004*\"sophomore\"\n",
      "INFO : topic #5 (0.067): 0.028*\"songs\" + 0.025*\"music\" + 0.018*\"song\" + 0.017*\"audience\" + 0.016*\"performance\" + 0.015*\"stage\" + 0.014*\"crowd\" + 0.012*\"band\" + 0.012*\"album\" + 0.011*\"concert\"\n",
      "INFO : topic #11 (0.067): 0.050*\"team\" + 0.028*\"season\" + 0.020*\"spring\" + 0.019*\"su\" + 0.018*\"tournament\" + 0.016*\"matches\" + 0.015*\"winning\" + 0.014*\"fall\" + 0.013*\"wrote\" + 0.012*\"singles\"\n",
      "INFO : topic #9 (0.067): 0.010*\"audience\" + 0.009*\"father\" + 0.008*\"performance\" + 0.007*\"killers\" + 0.007*\"socks\" + 0.007*\"belgian\" + 0.007*\"music\" + 0.007*\"dressed\" + 0.006*\"logic\" + 0.006*\"young\"\n",
      "INFO : topic #8 (0.067): 0.011*\"students\" + 0.008*\"professor\" + 0.007*\"student\" + 0.007*\"course\" + 0.006*\"work\" + 0.006*\"carter\" + 0.006*\"arts\" + 0.005*\"class\" + 0.005*\"program\" + 0.005*\"science\"\n",
      "INFO : topic diff=0.412474, rho=0.377843\n",
      "INFO : PROGRESS: pass 4, at document #4000/4009\n",
      "INFO : merging changes from 2000 documents into a model of 4009 documents\n",
      "INFO : topic #12 (0.067): 0.011*\"doo\" + 0.011*\"word\" + 0.011*\"kesha\" + 0.009*\"atlanta\" + 0.008*\"japanese\" + 0.008*\"school\" + 0.007*\"zoom\" + 0.006*\"star\" + 0.005*\"red\" + 0.005*\"rock\"\n",
      "INFO : topic #0 (0.067): 0.015*\"film\" + 0.004*\"world\" + 0.004*\"story\" + 0.004*\"love\" + 0.003*\"characters\" + 0.003*\"movie\" + 0.003*\"character\" + 0.003*\"life\" + 0.003*\"audience\" + 0.003*\"best\"\n",
      "INFO : topic #1 (0.067): 0.033*\"officer\" + 0.033*\"subject\" + 0.022*\"april\" + 0.020*\"complainant\" + 0.019*\"reported\" + 0.017*\"epd\" + 0.012*\"wallet\" + 0.012*\"assigned\" + 0.011*\"investigator\" + 0.011*\"case\"\n",
      "INFO : topic #10 (0.067): 0.016*\"law\" + 0.015*\"trump\" + 0.013*\"wax\" + 0.012*\"political\" + 0.009*\"students\" + 0.009*\"school\" + 0.007*\"action\" + 0.007*\"protesters\" + 0.006*\"right\" + 0.006*\"believe\"\n",
      "INFO : topic #11 (0.067): 0.050*\"team\" + 0.028*\"season\" + 0.019*\"tournament\" + 0.018*\"spring\" + 0.017*\"su\" + 0.015*\"matches\" + 0.014*\"winning\" + 0.013*\"singles\" + 0.013*\"fall\" + 0.012*\"play\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : topic diff=0.316222, rho=0.377843\n",
      "INFO : -6.559 per-word bound, 94.3 perplexity estimate based on a held-out corpus of 9 documents with 3490 words\n",
      "INFO : PROGRESS: pass 4, at document #4009/4009\n",
      "INFO : merging changes from 9 documents into a model of 4009 documents\n",
      "INFO : topic #9 (0.067): 0.010*\"audience\" + 0.010*\"father\" + 0.009*\"killers\" + 0.008*\"music\" + 0.008*\"socks\" + 0.008*\"belgian\" + 0.008*\"dressed\" + 0.007*\"performance\" + 0.007*\"wasn\" + 0.006*\"performed\"\n",
      "INFO : topic #11 (0.067): 0.050*\"team\" + 0.029*\"season\" + 0.022*\"spring\" + 0.022*\"su\" + 0.017*\"tournament\" + 0.016*\"matches\" + 0.016*\"winning\" + 0.015*\"fall\" + 0.015*\"wrote\" + 0.012*\"play\"\n",
      "INFO : topic #7 (0.067): 0.037*\"provost\" + 0.029*\"mcbride\" + 0.024*\"students\" + 0.022*\"faculty\" + 0.021*\"vice\" + 0.019*\"graduate\" + 0.018*\"office\" + 0.018*\"education\" + 0.013*\"hopes\" + 0.010*\"want\"\n",
      "INFO : topic #6 (0.067): 0.023*\"student\" + 0.021*\"sga\" + 0.010*\"president\" + 0.009*\"council\" + 0.009*\"epd\" + 0.007*\"cc\" + 0.007*\"board\" + 0.007*\"according\" + 0.006*\"responded\" + 0.006*\"elections\"\n",
      "INFO : topic #5 (0.067): 0.027*\"songs\" + 0.021*\"music\" + 0.016*\"audience\" + 0.016*\"performance\" + 0.013*\"stage\" + 0.012*\"crowd\" + 0.012*\"song\" + 0.010*\"band\" + 0.010*\"fans\" + 0.010*\"logic\"\n",
      "INFO : topic diff=0.624653, rho=0.377843\n",
      "INFO : LdaModel lifecycle event {'msg': 'trained LdaModel(num_terms=43158, num_topics=15, decay=0.5, chunksize=2000) in 43.67s', 'datetime': '2021-10-18T09:52:28.046530', 'gensim': '4.1.2', 'python': '3.7.2 (default, Dec 29 2018, 00:00:04) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50 s, sys: 4.61 s, total: 54.6 s\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "# run the model (using logging since it may take a while)\n",
    "# note the num_topics and the passes parameters; these are the most important parameters for topic modeling\n",
    "\n",
    "%time lda_model = gensim.models.LdaModel(wheel_corpus, num_topics=15, id2word=id2word_wheel, passes=5) \n",
    "\n",
    "# note that passes should be higher -- usually in the 50-100 range -- \n",
    "# but in the interests of time we'll only do 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpful functions for saving your model\n",
    "\n",
    "Because topic models can take a long time to run, it can be helpful to save your model and/or its components so that it can be loaded back in at a later date. Here's how you do those things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : storing corpus in Matrix Market format to ./wheel.corpus.mm\n",
      "INFO : saving sparse matrix to ./wheel.corpus.mm\n",
      "INFO : PROGRESS: saving document #0\n",
      "INFO : PROGRESS: saving document #1000\n",
      "INFO : PROGRESS: saving document #2000\n",
      "INFO : PROGRESS: saving document #3000\n",
      "INFO : PROGRESS: saving document #4000\n",
      "INFO : saved 4009x43158 matrix, density=0.527% (911829/173020422)\n",
      "INFO : saving MmCorpus index to ./wheel.corpus.mm.index\n",
      "INFO : Dictionary lifecycle event {'fname_or_handle': './wheel.dictionary', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-10-18T09:52:36.973549', 'gensim': '4.1.2', 'python': '3.7.2 (default, Dec 29 2018, 00:00:04) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'saving'}\n",
      "INFO : saved ./wheel.dictionary\n",
      "INFO : LdaState lifecycle event {'fname_or_handle': './lda_wheel-15topics_5iters.model.state', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-10-18T09:52:37.004142', 'gensim': '4.1.2', 'python': '3.7.2 (default, Dec 29 2018, 00:00:04) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'saving'}\n",
      "INFO : saved ./lda_wheel-15topics_5iters.model.state\n",
      "INFO : LdaModel lifecycle event {'fname_or_handle': './lda_wheel-15topics_5iters.model', 'separately': \"['expElogbeta', 'sstats']\", 'sep_limit': 10485760, 'ignore': ['state', 'dispatcher', 'id2word'], 'datetime': '2021-10-18T09:52:37.103264', 'gensim': '4.1.2', 'python': '3.7.2 (default, Dec 29 2018, 00:00:04) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'saving'}\n",
      "INFO : storing np array 'expElogbeta' to ./lda_wheel-15topics_5iters.model.expElogbeta.npy\n",
      "INFO : not storing attribute state\n",
      "INFO : not storing attribute dispatcher\n",
      "INFO : not storing attribute id2word\n",
      "INFO : saved ./lda_wheel-15topics_5iters.model\n"
     ]
    }
   ],
   "source": [
    "# how to store corpus to disk\n",
    "from gensim.corpora import MmCorpus\n",
    "MmCorpus.serialize('./wheel.corpus.mm', wheel_corpus) \n",
    "\n",
    "# how to store dictionary to disk\n",
    "id2word_wheel.save('./wheel.dictionary')\n",
    "\n",
    "# how to store model to disk \n",
    "lda_model.save('./lda_wheel-15topics_5iters.model')\n",
    "\n",
    "## IF THESE FUNCTIONS DON'T WORK IT'S LIKELY BECAUSE YOU DID NOT CLOSE-AND-HALT THEN REOPEN YOUR NOTEBOOK \n",
    "## IN YOUR MY-WORK FOLDER!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A helpful function for loading in a saved model\n",
    "\n",
    "You can also load in a saved model. \n",
    "\n",
    "Here, we're going to load in a (slightly) better topic model of the Emory Wheel with the same number of topics (15), but 50 iterations. I did not filter out any of the top n words, but I did filter out all of the words that appeared in only one article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading LdaModel object from ../models/lda_wheel-15topics_50iters.model\n",
      "INFO : loading expElogbeta from ../models/lda_wheel-15topics_50iters.model.expElogbeta.npy with mmap=None\n",
      "INFO : setting ignored attribute dispatcher to None\n",
      "INFO : setting ignored attribute id2word to None\n",
      "INFO : setting ignored attribute state to None\n",
      "INFO : LdaModel lifecycle event {'fname': '../models/lda_wheel-15topics_50iters.model', 'datetime': '2021-10-18T09:52:41.114783', 'gensim': '4.1.2', 'python': '3.7.2 (default, Dec 29 2018, 00:00:04) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'loaded'}\n",
      "INFO : loading LdaState object from ../models/lda_wheel-15topics_50iters.model.state\n",
      "INFO : LdaState lifecycle event {'fname': '../models/lda_wheel-15topics_50iters.model.state', 'datetime': '2021-10-18T09:52:41.169227', 'gensim': '4.1.2', 'python': '3.7.2 (default, Dec 29 2018, 00:00:04) \\n[Clang 4.0.1 (tags/RELEASE_401/final)]', 'platform': 'Darwin-20.6.0-x86_64-i386-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# load a saved model; in this case, a topic model of the ccp with 50 iterations\n",
    "lda_model = gensim.models.LdaModel.load('../models/lda_wheel-15topics_50iters.model')\n",
    "\n",
    "## IF YOU GET ERRORS HERE, CHECK YOUR PATH!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interacting with your model\n",
    "\n",
    "gensim comes with a bunch of built-in methods that make interacting with the output of the topic model a little easier. Here are some of the most useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"dairy\" + 0.018*\"fraternity\" + 0.017*\"greek\" + 0.015*\"house\" + 0.014*\"life\" + 0.013*\"cl\" + 0.012*\"sherlock\" + 0.012*\"mascot\" + 0.012*\"dangerously\" + 0.012*\"pill\"'),\n",
       " (1,\n",
       "  '0.017*\"emory\" + 0.016*\"students\" + 0.014*\"said\" + 0.010*\"student\" + 0.009*\"college\" + 0.009*\"president\" + 0.007*\"university\" + 0.007*\"sga\" + 0.006*\"according\" + 0.006*\"campus\"'),\n",
       " (2,\n",
       "  '0.015*\"emory\" + 0.014*\"said\" + 0.006*\"people\" + 0.005*\"atlanta\" + 0.005*\"according\" + 0.004*\"students\" + 0.004*\"new\" + 0.004*\"school\" + 0.004*\"time\" + 0.003*\"food\"'),\n",
       " (3,\n",
       "  '0.019*\"team\" + 0.017*\"emory\" + 0.014*\"game\" + 0.014*\"eagles\" + 0.012*\"said\" + 0.008*\"university\" + 0.008*\"season\" + 0.008*\"second\" + 0.007*\"time\" + 0.006*\"points\"'),\n",
       " (4,\n",
       "  '0.046*\"team\" + 0.027*\"season\" + 0.021*\"spring\" + 0.020*\"su\" + 0.016*\"matches\" + 0.015*\"said\" + 0.015*\"tournament\" + 0.015*\"winning\" + 0.015*\"fall\" + 0.014*\"wrote\"'),\n",
       " (5,\n",
       "  '0.017*\"provost\" + 0.017*\"said\" + 0.014*\"mcbride\" + 0.012*\"child\" + 0.012*\"university\" + 0.011*\"euh\" + 0.010*\"emory\" + 0.010*\"faculty\" + 0.010*\"office\" + 0.009*\"students\"'),\n",
       " (6,\n",
       "  '0.046*\"killers\" + 0.046*\"song\" + 0.043*\"album\" + 0.042*\"music\" + 0.029*\"songs\" + 0.028*\"rae\" + 0.026*\"band\" + 0.016*\"stage\" + 0.016*\"concert\" + 0.016*\"performance\"'),\n",
       " (7,\n",
       "  '0.017*\"israel\" + 0.008*\"adaptable\" + 0.008*\"fridayand\" + 0.008*\"united\" + 0.007*\"trump\" + 0.007*\"jewish\" + 0.007*\"israeli\" + 0.007*\"states\" + 0.007*\"country\" + 0.006*\"war\"'),\n",
       " (8,\n",
       "  '0.024*\"atlanta\" + 0.011*\"players\" + 0.010*\"week\" + 0.010*\"morris\" + 0.010*\"couldhave\" + 0.010*\"tocompete\" + 0.010*\"andare\" + 0.010*\"returners\" + 0.009*\"new\" + 0.009*\"season\"'),\n",
       " (9,\n",
       "  '0.017*\"trump\" + 0.016*\"law\" + 0.012*\"political\" + 0.009*\"school\" + 0.009*\"students\" + 0.009*\"university\" + 0.008*\"emory\" + 0.007*\"action\" + 0.007*\"protesters\" + 0.006*\"right\"'),\n",
       " (10,\n",
       "  '0.012*\"film\" + 0.009*\"like\" + 0.005*\"time\" + 0.005*\"world\" + 0.004*\"people\" + 0.004*\"story\" + 0.004*\"way\" + 0.004*\"love\" + 0.004*\"life\" + 0.003*\"new\"'),\n",
       " (11,\n",
       "  '0.017*\"doo\" + 0.014*\"like\" + 0.010*\"word\" + 0.010*\"japanese\" + 0.009*\"school\" + 0.009*\"emory\" + 0.009*\"zoom\" + 0.005*\"internet\" + 0.005*\"anti\" + 0.005*\"apple\"'),\n",
       " (12,\n",
       "  '0.017*\"father\" + 0.015*\"song\" + 0.013*\"dressed\" + 0.013*\"belgian\" + 0.013*\"music\" + 0.011*\"audience\" + 0.011*\"style\" + 0.011*\"wasn\" + 0.010*\"performed\" + 0.010*\"hats\"'),\n",
       " (13,\n",
       "  '0.028*\"officer\" + 0.026*\"subject\" + 0.019*\"said\" + 0.019*\"april\" + 0.019*\"epd\" + 0.017*\"reported\" + 0.015*\"complainant\" + 0.013*\"emory\" + 0.011*\"case\" + 0.011*\"individual\"'),\n",
       " (14,\n",
       "  '0.020*\"wax\" + 0.015*\"songs\" + 0.012*\"performance\" + 0.012*\"audience\" + 0.010*\"like\" + 0.010*\"logic\" + 0.009*\"music\" + 0.008*\"fans\" + 0.008*\"stage\" + 0.007*\"crowd\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the topics in the format (number of topics to show, number of terms)\n",
    "\n",
    "# as you can tell already, even the top words in each topic are only a very small proportion\n",
    "# of that topic, since we are dealing with about 40K unique words\n",
    "\n",
    "lda_model.show_topics(15, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T0: dairy, fraternity, greek, house, life, cl, sherlock, mascot, dangerously, pill, \n",
      "T1: emory, students, said, student, college, president, university, sga, according, campus, \n",
      "T2: emory, said, people, atlanta, according, students, new, school, time, food, \n",
      "T3: team, emory, game, eagles, said, university, season, second, time, points, \n",
      "T4: team, season, spring, su, matches, said, tournament, winning, fall, wrote, \n",
      "T5: provost, said, mcbride, child, university, euh, emory, faculty, office, students, \n",
      "T6: killers, song, album, music, songs, rae, band, stage, concert, performance, \n",
      "T7: israel, adaptable, fridayand, united, trump, jewish, israeli, states, country, war, \n",
      "T8: atlanta, players, week, morris, couldhave, tocompete, andare, returners, new, season, \n",
      "T9: trump, law, political, school, students, university, emory, action, protesters, right, \n",
      "T10: film, like, time, world, people, story, way, love, life, new, \n",
      "T11: doo, like, word, japanese, school, emory, zoom, internet, anti, apple, \n",
      "T12: father, song, dressed, belgian, music, audience, style, wasn, performed, hats, \n",
      "T13: officer, subject, said, april, epd, reported, complainant, emory, case, individual, \n",
      "T14: wax, songs, performance, audience, like, logic, music, fans, stage, crowd, \n"
     ]
    }
   ],
   "source": [
    "# let's format the words a little more nicely; \n",
    "# the formatted=False parameter returns tuples of (word, probability)\n",
    "\n",
    "topics = lda_model.show_topics(15, 10, formatted=False)\n",
    "\n",
    "for topic in topics:\n",
    "    topic_num = topic[0]\n",
    "    topic_words = \"\"\n",
    "    \n",
    "    topic_pairs = topic[1]\n",
    "    for pair in topic_pairs:\n",
    "        topic_words += pair[0] + \", \"\n",
    "    \n",
    "    print(\"T\" + str(topic_num) + \": \" + topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling topics\n",
    "\n",
    "Now you can see (I hope!) that what we call a \"topic\" is really just a list of the most probable words for that topic, which are sorted in descending order of probability. The most probable word for the topic is the first word. \n",
    "\n",
    "Topic models start to get more powerful when we, as human researchers, analyze the most probable words for every topic and summarize what these words have in common. This summary can then be used as a descriptive label for the topic. \n",
    "\n",
    "Remember, since an LDA topic model is an unsupervised algorithm, it doesn't know what these words mean in relationship to one another. It's up to us, as the human researchers, to make meaning out of the topics.\n",
    "\n",
    "## EXERCISE 1: LABELING TOPICS\n",
    "\n",
    "How might you label the following topics generated by the model above?\n",
    "\n",
    "✨Topic 3:✨\n",
    "\n",
    "`team, emory, game, eagles, said, university, season, second, time, points, senior, win, junior, year, sophomore, teams, freshman, college, run, play, `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here -- a single word or short phrase is fine\n",
    "\n",
    "ANSWER can be anything in the realm of: sports, athletics, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✨Topic 6:✨\n",
    "\n",
    "`killers, song, album, music, songs, rae, band, stage, concert, performance, crowd, audience, artists, lyrics, sound, dance, makeshift, collapsing, nextyear, weighty, `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here -- a single word or short phrase is fine\n",
    "\n",
    "ANSWER can be anything in the realm of: music, night life, concerts, performance, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✨Topic 13:✨\n",
    "\n",
    "`officer, subject, said, april, epd, reported, complainant, emory, case, individual, student, wallet, responded, assigned, investigator, number, second, stage, campus, driver, `\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here -- a single word or short phrase is fine\n",
    "\n",
    "ANSWER can be anything in the realm of: crime, police, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining the model\n",
    "\n",
    "These are decent topics, but they're not amazing. Here are a few things you might want to try in order to fine-tune your model:\n",
    "\n",
    "* Filtering some of the most common words (see the filtering function above)\n",
    "* Generating fewer topics (we could try 10, for instance). \n",
    "\n",
    "Most work with topic modeling involves a fair amount of trial and error before you arrive at an appropriate number of topics and the best ways to filter your corpus. \n",
    "\n",
    "Feel free to try those things on your own. \n",
    "\n",
    "### Topics and word probabilities\n",
    "\n",
    "Now let's take a bit of a closer look at the probabilities attached to each word in a single topic. We'll look at topic 13, the one that seems be about police and crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 13\n",
      "0. officer: 0.028290639\n",
      "1. subject: 0.026283318\n",
      "2. said: 0.019373914\n",
      "3. april: 0.01915113\n",
      "4. epd: 0.019024609\n",
      "5. reported: 0.016777173\n",
      "6. complainant: 0.015062824\n",
      "7. emory: 0.0134614585\n",
      "8. case: 0.010744892\n",
      "9. individual: 0.01052365\n"
     ]
    }
   ],
   "source": [
    "# T13 looks coherent\n",
    "topic = topics[13]\n",
    "\n",
    "print(\"Topic 13\")\n",
    "\n",
    "# the first item in the topic list is the topic number\n",
    "topic_num = topic[0]\n",
    "\n",
    "# the next item in the topic list is another list with pairs of words and percentages\n",
    "# this is what we want to examine\n",
    "topic_pairs = topic[1]\n",
    "for idx, pair in enumerate(topic_pairs):\n",
    "    print(str(idx) + \". \" + pair[0] + \": \" + str(pair[1]))\n",
    "\n",
    "# since all topics contain all words, the sum of all of the probabilities of each \n",
    "# topic should be 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 2: UNDERSTANDING WORD PROBABILITIES\n",
    "\n",
    "In a sentence or two, please explain what the output of the cell just above is telling us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# your answer here\n",
    "\n",
    "Some possible acceptable responses:\n",
    "\n",
    "- The 10 words that are most probable to occur in topic 13\n",
    "\n",
    "- The top 10 words associated with topic 13\n",
    "\n",
    "- A more detailed explaination like, \"the probability of the word \"officer\" appearing in topic 13 is .028290639 (or 2.8%), the probablity of the word \"subject\" appearing in topic 13 is 0.026283318 (or 2.6%) and so on....\"\n",
    "\n",
    "- Anything else that shows that they understand that these are specific words associated with topic 13. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents and topic probabilities / proportions\n",
    "\n",
    "Another way we can use the output of a topic model is to examine the probabilities of topics in each document. \n",
    "\n",
    "While MALLET provides this output automatically, there's a bit more work required to display it in gensim. When we're able to use MALLET later on in the semester, you will apprciate how much easier that is. \n",
    "\n",
    "For now, though, we'll do it this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1: 16.94% of document\n",
      "T2: 33.96% of document\n",
      "T3: 5.86% of document\n",
      "T5: 7.80% of document\n",
      "T7: 2.36% of document\n",
      "T8: 1.83% of document\n",
      "T10: 21.09% of document\n",
      "T12: 2.42% of document\n",
      "T13: 7.56% of document\n"
     ]
    }
   ],
   "source": [
    "tokens = [] \n",
    "\n",
    "# open one file\n",
    "with open('../corpora/emory-wheel/articles/2014-10-02-Atlanta-Food-Truck-Park-Brings-Enriching-Epicurian-Experience.txt', \"r\") as file:\n",
    "    text = file.read()\n",
    "    tokens = tokenize(text) # remember this from above\n",
    "\n",
    "# create the bag of words for the document on the basis of the Wheel dictionary, created above\n",
    "doc_bow = id2word_wheel.doc2bow(tokens)\n",
    "\n",
    "# get the topics that the doc consists of\n",
    "doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "\n",
    "# now, format this a bit more nicely so we can understand the output \n",
    "for topic, prob in doc_topics:\n",
    "    print(\"T\" + str(topic) + \": \" + \"{:.2%}\".format(prob) + \" of document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will note that this is not a list of all 15 topics. This is because, if a topic's proportion is very very small, it gets rounded down to zero and does not appear.\n",
    "\n",
    "Let's try to make this output even more meaningful by also displaying the words associated with each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T1: 16.94% of document. Top words: emory, students, said, student, college, president, university, sga, according, campus, \n",
      "T2: 33.96% of document. Top words: emory, said, people, atlanta, according, students, new, school, time, food, \n",
      "T3: 5.86% of document. Top words: team, emory, game, eagles, said, university, season, second, time, points, \n",
      "T5: 7.80% of document. Top words: provost, said, mcbride, child, university, euh, emory, faculty, office, students, \n",
      "T7: 2.36% of document. Top words: israel, adaptable, fridayand, united, trump, jewish, israeli, states, country, war, \n",
      "T8: 1.83% of document. Top words: atlanta, players, week, morris, couldhave, tocompete, andare, returners, new, season, \n",
      "T10: 21.09% of document. Top words: film, like, time, world, people, story, way, love, life, new, \n",
      "T12: 2.42% of document. Top words: father, song, dressed, belgian, music, audience, style, wasn, performed, hats, \n",
      "T13: 7.56% of document. Top words: officer, subject, said, april, epd, reported, complainant, emory, case, individual, \n"
     ]
    }
   ],
   "source": [
    "# cross-reference the topic proportions with the words to get more meaningful output\n",
    "for topic, prob in doc_topics:\n",
    "    topic_words = \"\"\n",
    "    select_topics = topics[topic]\n",
    "    \n",
    "    for pair in select_topics[1]:\n",
    "        topic_words += pair[0] + \", \"\n",
    "    \n",
    "    print(\"T\" + str(topic) + \": \" + \"{:.2%}\".format(prob) + \" of document. Top words: \" + topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 3: EXAMINING THE TOPICAL COMPOSITION OF DOCUMENTS\n",
    "\n",
    "Copying and modifying the code in the two cells above, print out the topical composition of another document in our corpus. (You may need to take a look at the directory that contains all of the articles to determine the name of a file to load). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T0: 1.49% of document\n",
      "T1: 30.68% of document\n",
      "T2: 22.94% of document\n",
      "T3: 7.72% of document\n",
      "T5: 4.99% of document\n",
      "T7: 2.86% of document\n",
      "T8: 1.10% of document\n",
      "T10: 24.63% of document\n",
      "T12: 1.49% of document\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "tokens = [] \n",
    "\n",
    "# open one file -- can be any file in the emory-wheel corpus EXCEPT the one used above about food trucks\n",
    "# here's one example:\n",
    "\n",
    "with open('../corpora/emory-wheel/articles/2014-10-02-Commission-Reports-on-Emory-Liberal-Arts.txt', \"r\") as file:\n",
    "    text = file.read()\n",
    "    tokens = tokenize(text) # remember this from above\n",
    "\n",
    "# create the bag of words for the document on the basis of the Wheel dictionary, created above\n",
    "doc_bow = id2word_wheel.doc2bow(tokens)\n",
    "\n",
    "# get the topics that the doc consists of\n",
    "doc_topics = lda_model.get_document_topics(doc_bow)\n",
    "\n",
    "# now, format this a bit more nicely so we can understand the output \n",
    "for topic, prob in doc_topics:\n",
    "    print(\"T\" + str(topic) + \": \" + \"{:.2%}\".format(prob) + \" of document\")\n",
    "\n",
    "# fine if they formatted it more nicely, or sorted it, or whatever, but also fine if not, e.g.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which topic appears in the highest proportion in your document? (In other words, which topic probability is most prevalent in the document you have selected?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here -- can just be the topic number. no need to write code to print it out.\n",
    "\n",
    "ANSWER can be anything like \"topic XX\" or \"TXX\" or anything that makes clear that they understand the meaning of the probabilities/percentages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Topics ###\n",
    "\n",
    "Gensim has several built-in methods for evaluating topics included as a model called [CoherenceModel](https://radimrehurek.com/gensim/models/coherencemodel.html). The fastest one to calculate is called u_mass, and in this case, the closer to zero (in either direction, positive or negative), the better the score. \n",
    "\n",
    "Let's see how our model performs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "cm = CoherenceModel(model=lda_model, corpus=wheel_corpus, coherence='u_mass')\n",
    "\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "\n",
    "coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with many metrics, this number is mostly helpful in a relative context. How some people employ metrics like this is to generate models with a variety of topics and look for the number of topics that yields the best coherence score. \n",
    "\n",
    "With that said, the debate about how best to evaluate topics is far from settled. Here's a review essay by Hanna Wallach et al. that summarizes a few additional methods of evaluation, including some involving humans in the loop: [\"Evaluation Methods for Topc Models\"](http://dirichlet.net/pdf/wallach09evaluation.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK. That's it. You did it! Time to upload your notebook to canvas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
