{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to scikit-learn (sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook was written in 2019 by Lauren Klein. It was updated in 2020 by Dan Sinykin, and again by Lauren Klein 2021.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, we've examined words in terms of their grammar and syntax. We've also looked at words in terms of various units: the word, the line, the document, etc. We've also touched on the idea of ngrams: short sequences of *n* words in a row (e.g. 2-grams or *bigrams*, 3-grams or *trigrams*, and so on).\n",
    "\n",
    "From here on out, however, we'll be taking a different approach. We'll be turning words into numbers, and then applying statistical measures and models to the numbers that represent the words. Things like tf-idf, topic modeling, BERT, similarity, classification, and clustering--the set of methods we'll be learning in the second half of the course--all rely on this basic transformation.\n",
    "\n",
    "While the actual transformation from words into numbers may be (relatively) easy to do, thanks to scikit-learn (or sklearn), it represents a major conceptual shift. For this reason, we're going to take some time to sit with it. For today, we'll just introduce ourselves to the concept via sk-learn, Python's major machine learning library which also happens to be crucial to many of the more advanced methods named above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a token? What is a feature? What is a document-term matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by importing sk-learn's `CountVectorizer`, which [converts a collection of text documents into a matrix of token counts](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). I've used the word \"token\" in passing before, but here I'll take a minute to formally define it, along with some related terms:\n",
    "\n",
    "According to the [Stanford NLP group](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html), a *token* is \"an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\" The unit of the token is usually the word, but it can also be the sentence, the subword, or anything else that makes sense for that particular task.\n",
    "\n",
    "Take this famous phrase, for example:\n",
    "\n",
    "\"To be or not to be\"\n",
    "\n",
    "This line has six tokens: \"to\", \"be\", \"or\", \"not\", \"to\", \"be\".\n",
    "\n",
    "It has four features: \"to\", \"be\", \"or\", \"not\"\n",
    "\n",
    "But wait, what is a feature?\n",
    "\n",
    "In this case, a *feature* is a unique token in the corpus. (Caveat: features, like tokens, can actually be anything that makes sense for the task, but for the purposes of turning words into numbers, features are most often unique words, or \"terms,\" as they're also sometimes called).\n",
    "\n",
    "When sk-learn's CountVectorizer does its thing, it first *tokenizes* all of the documents in the corpus--that is, it breaks up each document into its individual tokens--and then then creates a *document-term matrix* that counts up how many times each term, or feature, appears in each document. \n",
    "\n",
    "For example, the document-term matrix for the line above might look something like this:\n",
    "\n",
    "|   | to | be | or | not |\n",
    "|---|----|----|----|-----|\n",
    "|   | 2  | 2  | 1  | 1   |\n",
    "\n",
    "\n",
    "If we add in the second part of that phrase as a new document, we might get something like this:\n",
    "\n",
    "\n",
    "|         | to | be | or | not | that | is | the | question |\n",
    "|---------|----|----|----|-----|------|----|-----|----------|\n",
    "| line 1  | 2  | 2  | 1  | 1   | 0    | 0  | 0   | 0        |\n",
    "| line 2  | 0  | 0  | 0  | 0   | 1    | 1  | 0   | 0        |\n",
    "\n",
    "\n",
    "But enough of vectorizing by hand; let's try it out using sk-learn!\n",
    "\n",
    "\n",
    "## Importing sk-learn's CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CountVectorizer from sk-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize a teeny corpus\n",
    "\n",
    "Now let's vectorize a teeny corpus we can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's our corpus: the first stanza of \"Persimmons\" in which each line is its own document\n",
    "corpus = [\n",
    "    'In sixth grade Mrs. Walker',\n",
    "    'slapped the back of my head',\n",
    "    'and made me stand in the corner',\n",
    "    'for not knowing the difference',\n",
    "    'between persimmon and precision.',\n",
    "    'How to choose',\n",
    "]\n",
    "\n",
    "# instantiate the CountVectorizer object\n",
    "# note that this is the same conceptual process we used to instantiate\n",
    "# the VADER sentiment analysis object, and the spaCy document object\n",
    "cv=CountVectorizer()\n",
    "\n",
    "# this steps generates document-term matrix for the doc; \n",
    "# it's required before you do almost anything else\n",
    "dtm=cv.fit_transform(corpus)\n",
    "\n",
    "# this method gives us the feature names that the CountVectorizer vectorized:\n",
    "features = cv.get_feature_names()\n",
    "\n",
    "# this method turns our doc-term matrix into an array that can be manipulated:\n",
    "dtm_array = dtm.toarray()\n",
    "\n",
    "print(\"All of the features in our corpus:\")\n",
    "print(str(features))\n",
    "\n",
    "print (\"\\nAnd their counts in each of the \\\"documents,\\\" each of which is really just a single line of the poem:\")\n",
    "print(dtm_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is some code that uses dataframes to make the above slightly more legible\n",
    "# note that this is now the second or third time I've mentioned Python dataframes\n",
    "# and said we'll talk about them later--we will, promise!\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data=dtm_array,columns=features)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a minute to figure out what we're looking at:\n",
    "\n",
    "* Each column is a feature, or \"term,\" labeled with the name of the term, which in this case is a unique token\n",
    "* Each row is a document, labeled in order of being ingested\n",
    "* The \"1\" in row 0 of the \"grade\" column means that the term \"grade\" appears 1 time in the first document... and so on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing a corpus from a set of files\n",
    "\n",
    "The reality is that you almost always will be vectorizing a corpus from a set of files, and not a list that you type in by hand. This is how you'd do it with our song lyrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import this library for directory/file manipulation\n",
    "import os\n",
    "\n",
    "# set the base directory -- note that this may need to change if you've saved a copy\n",
    "# of this notebook elsewhere \n",
    "base_dir = \"../corpora/lyrics/\"\n",
    "\n",
    "# read in a list of all the filenames \n",
    "docs = os.listdir(base_dir)\n",
    "\n",
    "# a list for storing the text of all the docs\n",
    "all_docs = []\n",
    "\n",
    "# iterate through each of the docs in the directory\n",
    "for doc in docs:\n",
    "    with open(base_dir + doc, \"r\") as file:     # open the doc file \n",
    "        text = file.read()                      # read the contents of the file \n",
    "        all_docs.append(text)                   # append the contents of the file to our\n",
    "                                                # all_docs list for future vectorizing\n",
    "\n",
    "# just take a look at the first item to be sure it worked\n",
    "print(\"Filename: \" + str(docs[0]) + \"\\n\") \n",
    "print(all_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all that, the process of vectorizing the text of all the documents is the exact same one as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "cv=CountVectorizer()\n",
    "\n",
    "# generates document-term matrix for all the docs\n",
    "dtm=cv.fit_transform(all_docs)\n",
    "\n",
    "# get the feature names aka terms\n",
    "features = cv.get_feature_names()\n",
    "\n",
    "# take a look at the first 10 features\n",
    "print(features[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also check the overall shape of the doc-term matrix \n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does this tell you about how many documents there are?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What about the number of terms?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful CountVectorizer Parameters\n",
    "\n",
    "Here are a few more helpful CountVectorizer parameters to know about:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all words -- this is True by default, but if you want to preserve case,\n",
    "# you can set lowercase to False\n",
    "cv_caps = CountVectorizer(lowercase=False)\n",
    "\n",
    "# generates document-term matrix for all the docs\n",
    "dtm2=cv_caps.fit_transform(all_docs)\n",
    "\n",
    "# check the shape\n",
    "dtm2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are more terms since it's not merging the uppercase and the lowercase versions of each word together.\n",
    "\n",
    "Another parameter to know about has to do with stopwords. These are common words like \"and\", \"not\", \"or\", etc. that are not usually that interesting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the built-in English stopwords list\n",
    "cv_no_stops = CountVectorizer(stop_words='english')\n",
    "\n",
    "# generates document-term matrix for all the docs\n",
    "dtm3=cv_no_stops.fit_transform(all_docs)\n",
    "\n",
    "# check the shape\n",
    "dtm3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a custom stopwords list\n",
    "cv_no_stops = CountVectorizer(stop_words=['love','heart','star'])\n",
    "\n",
    "# generates document-term matrix for all the docs\n",
    "dtm4=cv_no_stops.fit_transform(all_docs)\n",
    "\n",
    "# check the shape\n",
    "dtm4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last helpful feature of CountVectorizer is that you can tell it very easily to tokenize by ngrams as well as words. To wit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_cv = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "\n",
    "# generates document-term matrix for all the docs\n",
    "dtm5=bigram_cv.fit_transform(all_docs)\n",
    "\n",
    "# get the feature names -- bigrams in this case\n",
    "features = bigram_cv.get_feature_names()\n",
    "\n",
    "# take a look at the first 10 features\n",
    "print(features[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the doc-term matrix to a dictionary\n",
    "\n",
    "Finally, here is some helpful code for creating a dictonary with the features as keys and the counts as values. Don't worry about being able to parse all the syntax unless you feel like it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here's our number of features\n",
    "num_feats = dtm.shape[1]\n",
    "\n",
    "# here's a dictionary to store the features and counts key/value pairs\n",
    "feature_dict = {}\n",
    "\n",
    "for x in range(num_feats):      # the for x in range() syntax is how you iterate over integers\n",
    "    key = cv.get_feature_names()[x]  # this gets the feature name at position [x]\n",
    "    value = dtm.toarray().sum(axis=0)[x]  # this sums the counts of the feature at\n",
    "                                          # position [x] for all documents\n",
    "    \n",
    "    feature_dict[key] = value # add the new key/value pair to the dictionary\n",
    "    \n",
    "# then sort the dictionary in order of counts\n",
    "sortFeats = sorted(feature_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# for more on the sorted function, see: https://www.w3schools.com/python/ref_func_sorted.asp\n",
    "# for more on lambda functions, see: https://towardsdatascience.com/lambda-functions-with-practical-examples-in-python-45934f3653a8\n",
    "\n",
    "# then print top 30\n",
    "\n",
    "for item in sortFeats[0:30]:\n",
    "    print(str(item[0]) + \": \" + str(item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also print from a bit lower in the counts\n",
    "\n",
    "for item in sortFeats[200:230]:\n",
    "    print(str(item[0]) + \": \" + str(item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How would you create a list of the top 30 words in our lyrics corpus but with stopwords removed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. That's it for today! scikit-learn and CountVectorizer set us up for the rest of the semester..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
